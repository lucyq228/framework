{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Selection Process 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# =========================================================\n",
    "# USER SETTINGS (edit these)\n",
    "# =========================================================\n",
    "STORE_COL = \"restaurant_id\"   # change to your store id column\n",
    "WEEK_COL = \"week_start\"       # change to your week column\n",
    "\n",
    "FEATURE_CATALOG_PATH = \"feature_catalog.csv\"  # your manually curated file\n",
    "\n",
    "# Correlation/VIF thresholds (simple defaults)\n",
    "HIGH_CORR_FLAG = 0.90\n",
    "MIN_FEATURES_PER_GROUP = 3\n",
    "\n",
    "VIF_MOD = 5.0\n",
    "VIF_HIGH = 10.0\n",
    "\n",
    "# Decision thresholds (simple + explainable)\n",
    "PAIR_EXPLOSION_THRESHOLD = 10         # if many pairs >= 0.90\n",
    "VIF_EXPLOSION_FRAC_THRESHOLD = 0.50   # if >=50% features have VIF>=10\n",
    "\n",
    "# For volatility flagging (per-feature)\n",
    "VOLATILITY_FLAG_THRESHOLD = 3.0  # median (within-store std / within-store mean_abs_change) heuristic, tune\n",
    "\n",
    "# =========================================================\n",
    "# Helpers: parsing\n",
    "# =========================================================\n",
    "def parse_bool(x):\n",
    "    if pd.isna(x): \n",
    "        return False\n",
    "    if isinstance(x, bool): \n",
    "        return x\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"true\", \"1\", \"yes\", \"y\", \"t\"}\n",
    "\n",
    "def parse_list_cell(x):\n",
    "    \"\"\"Excel/CSV-friendly: 'mobile,delivery' -> ['mobile','delivery'] ; blank -> []\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [v.strip() for v in s.split(\",\") if v.strip()]\n",
    "\n",
    "# =========================================================\n",
    "# Step A (simplified): Read feature catalog + validate\n",
    "# =========================================================\n",
    "def load_feature_catalog(path: str, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    fc = pd.read_csv(path)\n",
    "\n",
    "    required = {\"feature\", \"domain\", \"mechanism_group\", \"has_lag\"}\n",
    "    missing = required - set(fc.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"feature_catalog.csv missing required columns: {missing}\")\n",
    "\n",
    "    fc = fc.copy()\n",
    "    fc[\"has_lag\"] = fc[\"has_lag\"].apply(parse_bool)\n",
    "\n",
    "    if \"lag_weeks\" not in fc.columns:\n",
    "        fc[\"lag_weeks\"] = np.nan\n",
    "\n",
    "    # optional list-like columns\n",
    "    for col in [\"dayparts\", \"channels\", \"others\"]:\n",
    "        if col not in fc.columns:\n",
    "            fc[col] = \"\"\n",
    "        fc[col] = fc[col].apply(parse_list_cell)\n",
    "\n",
    "    # Keep only features that exist in data\n",
    "    fc[\"in_data\"] = fc[\"feature\"].isin(data.columns)\n",
    "    bad = fc.loc[~fc[\"in_data\"], \"feature\"].tolist()\n",
    "    if bad:\n",
    "        print(f\"[WARN] {len(bad)} features in catalog not found in data. Example: {bad[:10]}\")\n",
    "\n",
    "    fc = fc[fc[\"in_data\"]].reset_index(drop=True)\n",
    "\n",
    "    # Keep only numeric features for corr/VIF\n",
    "    numeric_cols = set(data.select_dtypes(include=[np.number]).columns)\n",
    "    fc[\"is_numeric\"] = fc[\"feature\"].isin(numeric_cols)\n",
    "    nonnum = fc.loc[~fc[\"is_numeric\"], \"feature\"].tolist()\n",
    "    if nonnum:\n",
    "        print(f\"[WARN] {len(nonnum)} features are not numeric; excluded from corr/VIF. Example: {nonnum[:10]}\")\n",
    "\n",
    "    fc = fc[fc[\"is_numeric\"]].reset_index(drop=True)\n",
    "    return fc\n",
    "\n",
    "# =========================================================\n",
    "# Feature-level quality metrics (simple + useful)\n",
    "# =========================================================\n",
    "def compute_feature_quality(data: pd.DataFrame, fc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Outputs per-feature:\n",
    "      - missing_rate\n",
    "      - store_availability_rate: % stores having at least one non-null observation\n",
    "      - volatility_score: median within-store week-to-week abs change / within-store mean abs level (rough)\n",
    "    \"\"\"\n",
    "    features = fc[\"feature\"].tolist()\n",
    "    out_rows = []\n",
    "\n",
    "    # Ensure sorted for time deltas\n",
    "    df = data[[STORE_COL, WEEK_COL] + features].copy()\n",
    "    df = df.sort_values([STORE_COL, WEEK_COL])\n",
    "\n",
    "    stores_total = df[STORE_COL].nunique()\n",
    "\n",
    "    for f in features:\n",
    "        s = df[f]\n",
    "        missing_rate = float(s.isna().mean())\n",
    "\n",
    "        # store availability: store has any non-null value\n",
    "        store_any = df.groupby(STORE_COL)[f].apply(lambda x: x.notna().any())\n",
    "        store_availability_rate = float(store_any.mean())  # fraction of stores\n",
    "\n",
    "        # volatility: within store week-to-week absolute change normalized by typical level\n",
    "        # (simple & robust; avoids model fitting)\n",
    "        # 1) compute diff per store\n",
    "        diff = df.groupby(STORE_COL)[f].diff().abs()\n",
    "        # 2) typical level (median abs value per store)\n",
    "        level = df.groupby(STORE_COL)[f].apply(lambda x: np.nanmedian(np.abs(x.values)))\n",
    "        # 3) typical diff (median abs diff per store)\n",
    "        diff_med = diff.groupby(df[STORE_COL]).apply(lambda x: np.nanmedian(x.values))\n",
    "\n",
    "        # avoid divide-by-zero\n",
    "        vol_per_store = (diff_med / (level.replace(0, np.nan)))\n",
    "        volatility_score = float(np.nanmedian(vol_per_store.values))\n",
    "\n",
    "        out_rows.append({\n",
    "            \"feature\": f,\n",
    "            \"missing_rate\": missing_rate,\n",
    "            \"store_availability_rate\": store_availability_rate,\n",
    "            \"volatility_score\": volatility_score,\n",
    "            \"volatility_flag\": (volatility_score >= VOLATILITY_FLAG_THRESHOLD) if np.isfinite(volatility_score) else False\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "# =========================================================\n",
    "# Step B: Group correlation matrices + summary\n",
    "# =========================================================\n",
    "def corr_summary_for_group(X: pd.DataFrame, high_corr_flag: float) -> dict:\n",
    "    corr = X.corr(method=\"pearson\")\n",
    "    # off-diagonal max abs corr\n",
    "    c = corr.copy()\n",
    "    np.fill_diagonal(c.values, 0.0)\n",
    "    abs_c = c.abs()\n",
    "\n",
    "    iu = np.triu_indices_from(abs_c.values, k=1)\n",
    "    vals = abs_c.values[iu]\n",
    "    max_abs = float(np.nanmax(vals)) if len(vals) else np.nan\n",
    "    n_pairs_flagged = int(np.sum(vals >= high_corr_flag)) if len(vals) else 0\n",
    "\n",
    "    # keep some top pairs for debugging\n",
    "    top_pairs = []\n",
    "    if len(vals):\n",
    "        top_idx = np.argsort(vals)[::-1][:min(30, len(vals))]\n",
    "        cols = abs_c.columns.to_list()\n",
    "        for idx in top_idx:\n",
    "            i, j = iu[0][idx], iu[1][idx]\n",
    "            top_pairs.append((cols[i], cols[j], float(vals[idx])))\n",
    "    top_pairs_df = pd.DataFrame(top_pairs, columns=[\"feature_a\", \"feature_b\", \"abs_corr\"])\n",
    "\n",
    "    return {\n",
    "        \"corr_pearson\": corr,\n",
    "        \"max_abs_corr_offdiag\": max_abs,\n",
    "        \"n_pairs_flagged_ge_0p90\": n_pairs_flagged,\n",
    "        \"top_abs_corr_pairs\": top_pairs_df,\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# Step D: VIF within group (simple, stable)\n",
    "# =========================================================\n",
    "def zscore_df(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    mu = X.mean(axis=0, skipna=True)\n",
    "    sd = X.std(axis=0, ddof=0, skipna=True).replace(0, np.nan)\n",
    "    return (X - mu) / sd\n",
    "\n",
    "def vif_from_corr(C: np.ndarray, names: list, ridge_eps=1e-8) -> pd.Series:\n",
    "    C = np.nan_to_num(C, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    C = (C + C.T) / 2.0\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    C = C + np.eye(C.shape[0]) * ridge_eps\n",
    "    try:\n",
    "        invC = np.linalg.inv(C)\n",
    "    except np.linalg.LinAlgError:\n",
    "        invC = np.linalg.pinv(C)\n",
    "    return pd.Series(np.diag(invC), index=names)\n",
    "\n",
    "def vif_summary_for_group(X: pd.DataFrame) -> dict:\n",
    "    # Drop near-constant columns\n",
    "    nunique = X.nunique(dropna=True)\n",
    "    var = X.var(numeric_only=True)\n",
    "    keep_cols = X.columns[(nunique > 2) & (var > 1e-12)]\n",
    "    X = X[keep_cols].copy()\n",
    "\n",
    "    if X.shape[1] < 2:\n",
    "        return {\"vif_table\": pd.DataFrame(), \"vif_max\": np.nan, \"vif_p95\": np.nan, \"vif_median\": np.nan,\n",
    "                \"n_vif_ge_10\": 0, \"n_vif_ge_5\": 0, \"top_vif_feature\": None, \"top_vif_value\": np.nan}\n",
    "\n",
    "    # For VIF we need a reasonable number of rows; use rows with not-too-much missingness\n",
    "    row_missing = X.isna().mean(axis=1)\n",
    "    X = X.loc[row_missing <= 0.5]\n",
    "    if len(X) < 100:\n",
    "        return {\"vif_table\": pd.DataFrame(), \"vif_max\": np.nan, \"vif_p95\": np.nan, \"vif_median\": np.nan,\n",
    "                \"n_vif_ge_10\": 0, \"n_vif_ge_5\": 0, \"top_vif_feature\": None, \"top_vif_value\": np.nan}\n",
    "\n",
    "    Z = zscore_df(X)\n",
    "    Z = Z.loc[:, Z.notna().any(axis=0)]\n",
    "    if Z.shape[1] < 2:\n",
    "        return {\"vif_table\": pd.DataFrame(), \"vif_max\": np.nan, \"vif_p95\": np.nan, \"vif_median\": np.nan,\n",
    "                \"n_vif_ge_10\": 0, \"n_vif_ge_5\": 0, \"top_vif_feature\": None, \"top_vif_value\": np.nan}\n",
    "\n",
    "    C = Z.corr(method=\"pearson\").to_numpy()\n",
    "    vif = vif_from_corr(C, Z.columns.to_list())\n",
    "\n",
    "    vif_tbl = pd.DataFrame({\"feature\": vif.index, \"vif\": vif.values}).sort_values(\"vif\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    vif_max = float(vif_tbl[\"vif\"].max())\n",
    "    vif_p95 = float(vif_tbl[\"vif\"].quantile(0.95))\n",
    "    vif_median = float(vif_tbl[\"vif\"].median())\n",
    "    n_ge_10 = int((vif_tbl[\"vif\"] >= VIF_HIGH).sum())\n",
    "    n_ge_5 = int((vif_tbl[\"vif\"] >= VIF_MOD).sum())\n",
    "    top_feat = str(vif_tbl.iloc[0][\"feature\"])\n",
    "    top_val = float(vif_tbl.iloc[0][\"vif\"])\n",
    "\n",
    "    return {\n",
    "        \"vif_table\": vif_tbl,\n",
    "        \"vif_max\": vif_max,\n",
    "        \"vif_p95\": vif_p95,\n",
    "        \"vif_median\": vif_median,\n",
    "        \"n_vif_ge_10\": n_ge_10,\n",
    "        \"n_vif_ge_5\": n_ge_5,\n",
    "        \"top_vif_feature\": top_feat,\n",
    "        \"top_vif_value\": top_val,\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# Step E (simplified): decision logic + optional collapse\n",
    "# =========================================================\n",
    "def decide_group(n_features: int,\n",
    "                 max_abs_corr_offdiag: float,\n",
    "                 n_pairs_flagged_ge_0p90: int,\n",
    "                 vif_max: float,\n",
    "                 n_vif_ge_10: int) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Returns (decision, rationale).\n",
    "    Decisions:\n",
    "      - KEEP: keep as-is\n",
    "      - KEEP_REPS: keep 1 rep per high-corr cluster (simple)\n",
    "      - COLLAPSE_TO_INDEX: create one index feature (simple mean)\n",
    "    \"\"\"\n",
    "    # compute fraction high VIF\n",
    "    frac_vif_ge_10 = (n_vif_ge_10 / n_features) if n_features else 0.0\n",
    "\n",
    "    # Explosion triggers\n",
    "    if (n_pairs_flagged_ge_0p90 >= PAIR_EXPLOSION_THRESHOLD) or (frac_vif_ge_10 >= VIF_EXPLOSION_FRAC_THRESHOLD):\n",
    "        return (\"COLLAPSE_TO_INDEX\",\n",
    "                f\"High redundancy: pairs>=0.90={n_pairs_flagged_ge_0p90}, frac(VIF>=10)={frac_vif_ge_10:.2f}. Collapse to index.\")\n",
    "\n",
    "    # Moderate redundancy triggers\n",
    "    if (max_abs_corr_offdiag is not None and max_abs_corr_offdiag >= HIGH_CORR_FLAG) or (vif_max is not None and vif_max >= VIF_HIGH):\n",
    "        return (\"KEEP_REPS\",\n",
    "                f\"Moderate/high redundancy signal: max_abs_corr={max_abs_corr_offdiag:.3f}, vif_max={vif_max:.1f}. Keep representatives per cluster.\")\n",
    "\n",
    "    return (\"KEEP\",\n",
    "            f\"Low redundancy: max_abs_corr={max_abs_corr_offdiag:.3f}, vif_max={vif_max:.1f}. Keep group features.\")\n",
    "\n",
    "def choose_rep_simple(features: list[str], feature_quality: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Simple representative choice:\n",
    "      - lowest missing_rate\n",
    "      - highest store_availability\n",
    "      - lowest volatility_score\n",
    "    \"\"\"\n",
    "    q = feature_quality.set_index(\"feature\").loc[features].copy()\n",
    "    q[\"missing_rate\"] = q[\"missing_rate\"].fillna(1.0)\n",
    "    q[\"store_availability_rate\"] = q[\"store_availability_rate\"].fillna(0.0)\n",
    "    q[\"volatility_score\"] = q[\"volatility_score\"].fillna(np.inf)\n",
    "\n",
    "    q = q.sort_values(\n",
    "        [\"missing_rate\", \"store_availability_rate\", \"volatility_score\"],\n",
    "        ascending=[True, False, True]\n",
    "    )\n",
    "    return q.index[0]\n",
    "\n",
    "# Very simple clustering by correlation adjacency (connected components)\n",
    "def corr_clusters(corr_abs: pd.DataFrame, threshold: float) -> list[list[str]]:\n",
    "    cols = corr_abs.columns.tolist()\n",
    "    adj = {c: set() for c in cols}\n",
    "    M = corr_abs.values\n",
    "    n = len(cols)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if M[i, j] >= threshold:\n",
    "                a, b = cols[i], cols[j]\n",
    "                adj[a].add(b); adj[b].add(a)\n",
    "    seen = set()\n",
    "    comps = []\n",
    "    for c in cols:\n",
    "        if c in seen:\n",
    "            continue\n",
    "        stack = [c]\n",
    "        seen.add(c)\n",
    "        comp = []\n",
    "        while stack:\n",
    "            u = stack.pop()\n",
    "            comp.append(u)\n",
    "            for v in adj[u]:\n",
    "                if v not in seen:\n",
    "                    seen.add(v)\n",
    "                    stack.append(v)\n",
    "        comps.append(sorted(comp))\n",
    "    return comps\n",
    "\n",
    "# =========================================================\n",
    "# MAIN: run simplified pipeline B + D + E\n",
    "# =========================================================\n",
    "def run_simple_pipeline(data: pd.DataFrame, feature_catalog_csv: str):\n",
    "    # Load catalog\n",
    "    fc = load_feature_catalog(feature_catalog_csv, data)\n",
    "\n",
    "    # Feature quality\n",
    "    feature_quality = compute_feature_quality(data, fc)\n",
    "\n",
    "    # Build groups\n",
    "    group_keys = [\"domain\", \"mechanism_group\", \"has_lag\", \"lag_weeks\"]\n",
    "    grouped = fc.groupby(group_keys, dropna=False)\n",
    "\n",
    "    covariate_matrices = {}  # group_key -> {corr_pearson, top pairs, ...}\n",
    "    vif_details = {}         # group_key -> vif_table\n",
    "    summary_rows = []\n",
    "    mapping_rows = []\n",
    "    collapse_definitions = {}\n",
    "\n",
    "    # Iterate each group\n",
    "    for gkey, gdf in grouped:\n",
    "        features = gdf[\"feature\"].tolist()\n",
    "        if len(features) < MIN_FEATURES_PER_GROUP:\n",
    "            continue\n",
    "\n",
    "        X = data[features]\n",
    "\n",
    "        # Step B: correlation\n",
    "        corr_out = corr_summary_for_group(X, HIGH_CORR_FLAG)\n",
    "        covariate_matrices[gkey] = corr_out\n",
    "\n",
    "        # Step D: VIF\n",
    "        vif_out = vif_summary_for_group(X)\n",
    "        vif_details[gkey] = vif_out[\"vif_table\"]\n",
    "\n",
    "        # Step E: decision\n",
    "        decision, rationale = decide_group(\n",
    "            n_features=len(features),\n",
    "            max_abs_corr_offdiag=corr_out[\"max_abs_corr_offdiag\"],\n",
    "            n_pairs_flagged_ge_0p90=corr_out[\"n_pairs_flagged_ge_0p90\"],\n",
    "            vif_max=vif_out[\"vif_max\"],\n",
    "            n_vif_ge_10=vif_out[\"n_vif_ge_10\"]\n",
    "        )\n",
    "\n",
    "        # Quality summaries at group level\n",
    "        fq = feature_quality.set_index(\"feature\").loc[features]\n",
    "        miss_mean = float(fq[\"missing_rate\"].mean())\n",
    "        avail_mean = float(fq[\"store_availability_rate\"].mean())\n",
    "        vol_med = float(np.nanmedian(fq[\"volatility_score\"].values))\n",
    "\n",
    "        # Apply the decision (simple actions)\n",
    "        selected = []\n",
    "        if decision == \"KEEP\":\n",
    "            selected = features\n",
    "            for f in features:\n",
    "                mapping_rows.append({\"group_key\": gkey, \"feature\": f, \"action\": \"KEEP\", \"mapped_to\": f})\n",
    "\n",
    "        elif decision == \"KEEP_REPS\":\n",
    "            # cluster by corr >= 0.90 and keep one rep per cluster\n",
    "            corr_abs = corr_out[\"corr_pearson\"].abs()\n",
    "            clusters = corr_clusters(corr_abs, HIGH_CORR_FLAG)\n",
    "            for ci, cl in enumerate(clusters):\n",
    "                rep = choose_rep_simple(cl, feature_quality)\n",
    "                selected.append(rep)\n",
    "                for f in cl:\n",
    "                    mapping_rows.append({\"group_key\": gkey, \"feature\": f, \"action\": \"REP\" if f == rep else \"MAP_TO_REP\", \"mapped_to\": rep})\n",
    "\n",
    "        elif decision == \"COLLAPSE_TO_INDEX\":\n",
    "            # create one simple mean index (exec-friendly)\n",
    "            index_name = f\"{gkey[0]}__{gkey[1]}__{'lag'+str(int(gkey[3])) if parse_bool(gkey[2]) else 'nolag'}__idx\"\n",
    "            data[index_name] = data[features].mean(axis=1, skipna=True)\n",
    "\n",
    "            selected = [index_name]\n",
    "            collapse_definitions[index_name] = {\n",
    "                \"type\": \"mean_index\",\n",
    "                \"group_key\": str(gkey),\n",
    "                \"inputs\": features\n",
    "            }\n",
    "            for f in features:\n",
    "                mapping_rows.append({\"group_key\": gkey, \"feature\": f, \"action\": \"COLLAPSE_TO_INDEX\", \"mapped_to\": index_name})\n",
    "\n",
    "        # Build summary row (what you asked for)\n",
    "        summary_rows.append({\n",
    "            \"group_key\": gkey,\n",
    "            \"domain\": gkey[0],\n",
    "            \"mechanism_group\": gkey[1],\n",
    "            \"has_lag\": gkey[2],\n",
    "            \"lag_weeks\": gkey[3],\n",
    "            \"n_features\": len(features),\n",
    "\n",
    "            # feature quality summary\n",
    "            \"missing_rate_mean\": miss_mean,\n",
    "            \"store_availability_rate_mean\": avail_mean,\n",
    "            \"volatility_score_median\": vol_med,\n",
    "\n",
    "            # corr summary\n",
    "            \"max_abs_corr_offdiag\": corr_out[\"max_abs_corr_offdiag\"],\n",
    "            \"n_pairs_flagged_ge_0p90\": corr_out[\"n_pairs_flagged_ge_0p90\"],\n",
    "\n",
    "            # vif summary\n",
    "            \"vif_max\": vif_out[\"vif_max\"],\n",
    "            \"vif_p95\": vif_out[\"vif_p95\"],\n",
    "            \"vif_median\": vif_out[\"vif_median\"],\n",
    "            \"n_vif_ge_10\": vif_out[\"n_vif_ge_10\"],\n",
    "            \"n_vif_ge_5\": vif_out[\"n_vif_ge_5\"],\n",
    "            \"top_vif_feature\": vif_out[\"top_vif_feature\"],\n",
    "            \"top_vif_value\": vif_out[\"top_vif_value\"],\n",
    "\n",
    "            # decision\n",
    "            \"decision\": decision,\n",
    "            \"rationale\": rationale,\n",
    "\n",
    "            # what got selected\n",
    "            \"n_selected_from_group\": len(selected),\n",
    "            \"selected_preview\": \", \".join(selected[:5]) + (\"...\" if len(selected) > 5 else \"\")\n",
    "        })\n",
    "\n",
    "    summary = pd.DataFrame(summary_rows).sort_values(\n",
    "        [\"decision\", \"n_features\", \"n_pairs_flagged_ge_0p90\", \"vif_max\"],\n",
    "        ascending=[True, False, False, False]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    mapping_table = pd.DataFrame(mapping_rows)\n",
    "\n",
    "    # Final selected set:\n",
    "    final_selected = set(mapping_table.loc[mapping_table[\"action\"].isin([\"KEEP\", \"REP\"]), \"mapped_to\"].tolist())\n",
    "    final_selected |= set(mapping_table.loc[mapping_table[\"action\"] == \"COLLAPSE_TO_INDEX\", \"mapped_to\"].tolist())\n",
    "\n",
    "    # add IDs\n",
    "    for col in [STORE_COL, WEEK_COL]:\n",
    "        if col in data.columns:\n",
    "            final_selected.add(col)\n",
    "\n",
    "    final_selected = sorted(final_selected)\n",
    "\n",
    "    return {\n",
    "        \"summary\": summary,\n",
    "        \"mapping_table\": mapping_table,\n",
    "        \"collapse_definitions\": collapse_definitions,\n",
    "        \"final_selected_features\": final_selected,\n",
    "        \"feature_quality\": feature_quality,\n",
    "        \"covariate_matrices\": covariate_matrices,  # for detailed lookups\n",
    "        \"vif_details\": vif_details,                # per-group VIF tables\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# RUN\n",
    "# =========================================================\n",
    "# Example usage:\n",
    "# data = pd.read_parquet(\"your_store_week_data.parquet\")  # or however you load\n",
    "results = run_simple_pipeline(data, FEATURE_CATALOG_PATH)\n",
    "\n",
    "summary = results[\"summary\"]\n",
    "mapping_table = results[\"mapping_table\"]\n",
    "final_features = results[\"final_selected_features\"]\n",
    "\n",
    "print(\"Final selected features:\", len(final_features))\n",
    "print(summary.head(20))\n",
    "\n",
    "# =========================================================\n",
    "# HOW TO PULL DETAILS (if someone asks)\n",
    "# =========================================================\n",
    "# Pick a group_key from summary:\n",
    "# gkey = summary.loc[0, \"group_key\"]\n",
    "# corr_matrix = results[\"covariate_matrices\"][gkey][\"corr_pearson\"]\n",
    "# top_pairs = results[\"covariate_matrices\"][gkey][\"top_abs_corr_pairs\"]\n",
    "# vif_table = results[\"vif_details\"][gkey]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
