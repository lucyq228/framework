{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLflow Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, math, tempfile\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Iterator, Tuple, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
        "\n",
        "import mlflow\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# import mlflow\n",
        "\n",
        "TRACK_DIR = os.path.abspath(\"./mlruns_rga_test\")\n",
        "mlflow.set_tracking_uri(f\"file:///{TRACK_DIR.replace(os.sep, '/')}\") # for local testing. not needed for Databricks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def make_tiny_df():\n",
        "    weeks = pd.date_range(\"2023-01-01\", \"2025-06-30\", freq=\"W-SUN\")\n",
        "    n_weeks = len(weeks)\n",
        "    store_ids = list(range(1, 11))  # 10 stores: 1, 2, ..., 10\n",
        "    n_stores = len(store_ids)\n",
        "\n",
        "    rng = np.random.default_rng(42)\n",
        "    n = n_weeks * n_stores\n",
        "\n",
        "    # Each store has the same week_start: cross product of store_id x week_start\n",
        "    df = pd.DataFrame({\n",
        "        \"store_id\": np.repeat(store_ids, n_weeks),\n",
        "        \"week_start\": np.tile(weeks, n_stores),\n",
        "        # target: raw GC (no log)\n",
        "        \"GC\": rng.uniform(100, 10000, n),\n",
        "    })\n",
        "\n",
        "    # digital_promo features: within 0-1\n",
        "    for c in ['digital_promo_1', 'digital_promo_2', 'digital_promo_3', 'digital_promo_4', 'digital_promo_5']:\n",
        "        df[c] = rng.uniform(0, 1, n)\n",
        "\n",
        "    # media features: within 100-100000\n",
        "    for c in ['media_1', 'media_2', 'media_3', 'media_4', 'media_5']:\n",
        "        df[c] = rng.uniform(100, 100_000, n)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = make_tiny_df()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Log strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "\n",
        "def _compute_log_offset(\n",
        "    s: pd.Series,\n",
        "    strategy: str = \"one\",\n",
        "    eps: float = 1e-6,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Determine offset for log transform.\n",
        "    \n",
        "    strategy:\n",
        "      - \"one\": offset = 1.0\n",
        "      - \"median\": offset = eps * median(positive values)\n",
        "    \"\"\"\n",
        "    if strategy == \"one\":\n",
        "        return 1.0\n",
        "\n",
        "    if strategy == \"median\":\n",
        "        pos = s[s > 0]\n",
        "        if len(pos) == 0:\n",
        "            return 1.0\n",
        "        return eps * float(pos.median())\n",
        "\n",
        "    raise ValueError(f\"Unknown offset strategy: {strategy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_log_features(\n",
        "    df: pd.DataFrame,\n",
        "    features: List[str],\n",
        "    offset_strategy: str = \"one\",\n",
        "    eps: float = 1e-6,\n",
        "    drop_negative: bool = False,\n",
        ") -> Tuple[pd.DataFrame, Dict[str, Dict]]:\n",
        "    \"\"\"\n",
        "    Create log-transformed features safely.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas DataFrame\n",
        "    features : list of column names to transform\n",
        "    offset_strategy : \"one\" or \"median\"\n",
        "    eps : used only if offset_strategy=\"median\"\n",
        "    drop_negative : if True, negative values are set to NaN\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df_out : DataFrame with new log_{feature} columns\n",
        "    log_metadata : dict keyed by feature with transformation stats\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "    log_metadata = {}\n",
        "\n",
        "    for f in features:\n",
        "        if f not in df_out.columns:\n",
        "            raise ValueError(f\"Feature '{f}' not found in DataFrame\")\n",
        "\n",
        "        s = df_out[f].astype(float)\n",
        "\n",
        "        meta = {\n",
        "            \"raw_feature\": f,\n",
        "            \"n_rows\": len(s),\n",
        "            \"n_missing\": int(s.isna().sum()),\n",
        "            \"n_zero\": int((s == 0).sum()),\n",
        "            \"n_negative\": int((s < 0).sum()),\n",
        "        }\n",
        "\n",
        "        # handle negatives\n",
        "        if drop_negative:\n",
        "            s = s.mask(s < 0)\n",
        "\n",
        "        # compute offset\n",
        "        offset = _compute_log_offset(s, strategy=offset_strategy, eps=eps)\n",
        "        meta[\"log_offset\"] = offset\n",
        "        meta[\"offset_strategy\"] = offset_strategy\n",
        "\n",
        "        # safe log\n",
        "        log_col = f\"log_{f}\"\n",
        "        df_out[log_col] = np.log(s + offset)\n",
        "\n",
        "        # infinities → NaN\n",
        "        df_out[log_col] = df_out[log_col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        meta[\"n_log_missing\"] = int(df_out[log_col].isna().sum())\n",
        "\n",
        "        log_metadata[f] = meta\n",
        "\n",
        "    return df_out, log_metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_features = ['GC',\n",
        "              'media_1','media_2','media_3','media_4','media_5'\n",
        "]\n",
        "\n",
        "df, log_meta = add_log_features(\n",
        "    df=df,\n",
        "    features=log_features,\n",
        "    offset_strategy=\"one\",   # stable, interpretable\n",
        "    drop_negative=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>store_id</th>\n",
              "      <th>week_start</th>\n",
              "      <th>GC</th>\n",
              "      <th>digital_promo_1</th>\n",
              "      <th>digital_promo_2</th>\n",
              "      <th>digital_promo_3</th>\n",
              "      <th>digital_promo_4</th>\n",
              "      <th>digital_promo_5</th>\n",
              "      <th>media_1</th>\n",
              "      <th>media_2</th>\n",
              "      <th>media_3</th>\n",
              "      <th>media_4</th>\n",
              "      <th>media_5</th>\n",
              "      <th>log_GC</th>\n",
              "      <th>log_media_1</th>\n",
              "      <th>log_media_2</th>\n",
              "      <th>log_media_3</th>\n",
              "      <th>log_media_4</th>\n",
              "      <th>log_media_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2023-01-01</td>\n",
              "      <td>7762.164881</td>\n",
              "      <td>0.392485</td>\n",
              "      <td>0.877477</td>\n",
              "      <td>0.531428</td>\n",
              "      <td>0.138708</td>\n",
              "      <td>0.863492</td>\n",
              "      <td>61326.061549</td>\n",
              "      <td>17201.343184</td>\n",
              "      <td>97417.410477</td>\n",
              "      <td>71648.944633</td>\n",
              "      <td>3607.031881</td>\n",
              "      <td>8.957145</td>\n",
              "      <td>11.023976</td>\n",
              "      <td>9.752801</td>\n",
              "      <td>11.486770</td>\n",
              "      <td>11.179548</td>\n",
              "      <td>8.190918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2023-01-08</td>\n",
              "      <td>4444.896554</td>\n",
              "      <td>0.923900</td>\n",
              "      <td>0.959596</td>\n",
              "      <td>0.586109</td>\n",
              "      <td>0.293273</td>\n",
              "      <td>0.240285</td>\n",
              "      <td>3988.161026</td>\n",
              "      <td>67742.162031</td>\n",
              "      <td>5269.544085</td>\n",
              "      <td>62586.983836</td>\n",
              "      <td>16846.864943</td>\n",
              "      <td>8.399737</td>\n",
              "      <td>8.291336</td>\n",
              "      <td>11.123479</td>\n",
              "      <td>8.569889</td>\n",
              "      <td>11.044329</td>\n",
              "      <td>9.731979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2023-01-15</td>\n",
              "      <td>8600.119407</td>\n",
              "      <td>0.578978</td>\n",
              "      <td>0.228336</td>\n",
              "      <td>0.510166</td>\n",
              "      <td>0.305943</td>\n",
              "      <td>0.839874</td>\n",
              "      <td>12813.036067</td>\n",
              "      <td>2574.355308</td>\n",
              "      <td>79595.063348</td>\n",
              "      <td>65263.859444</td>\n",
              "      <td>41218.643405</td>\n",
              "      <td>9.059648</td>\n",
              "      <td>9.458296</td>\n",
              "      <td>7.853743</td>\n",
              "      <td>11.284720</td>\n",
              "      <td>11.086209</td>\n",
              "      <td>10.626670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2023-01-22</td>\n",
              "      <td>7003.943488</td>\n",
              "      <td>0.004619</td>\n",
              "      <td>0.931824</td>\n",
              "      <td>0.036025</td>\n",
              "      <td>0.261298</td>\n",
              "      <td>0.078545</td>\n",
              "      <td>50045.750273</td>\n",
              "      <td>16101.731418</td>\n",
              "      <td>64607.084138</td>\n",
              "      <td>17677.173940</td>\n",
              "      <td>71819.519391</td>\n",
              "      <td>8.854371</td>\n",
              "      <td>10.820713</td>\n",
              "      <td>9.686744</td>\n",
              "      <td>11.076095</td>\n",
              "      <td>9.780086</td>\n",
              "      <td>11.181925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2023-01-29</td>\n",
              "      <td>1032.355744</td>\n",
              "      <td>0.038542</td>\n",
              "      <td>0.571213</td>\n",
              "      <td>0.497091</td>\n",
              "      <td>0.460761</td>\n",
              "      <td>0.159928</td>\n",
              "      <td>74659.258456</td>\n",
              "      <td>90270.550139</td>\n",
              "      <td>26217.482191</td>\n",
              "      <td>66950.482733</td>\n",
              "      <td>36412.155677</td>\n",
              "      <td>6.940567</td>\n",
              "      <td>11.220703</td>\n",
              "      <td>11.410578</td>\n",
              "      <td>10.174220</td>\n",
              "      <td>11.111723</td>\n",
              "      <td>10.502685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1305</th>\n",
              "      <td>10</td>\n",
              "      <td>2025-06-01</td>\n",
              "      <td>9282.298110</td>\n",
              "      <td>0.494260</td>\n",
              "      <td>0.560057</td>\n",
              "      <td>0.626922</td>\n",
              "      <td>0.845786</td>\n",
              "      <td>0.792066</td>\n",
              "      <td>39787.334767</td>\n",
              "      <td>6773.695983</td>\n",
              "      <td>913.135423</td>\n",
              "      <td>53610.359616</td>\n",
              "      <td>88195.150862</td>\n",
              "      <td>9.135972</td>\n",
              "      <td>10.591329</td>\n",
              "      <td>8.820950</td>\n",
              "      <td>6.817979</td>\n",
              "      <td>10.889516</td>\n",
              "      <td>11.387319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306</th>\n",
              "      <td>10</td>\n",
              "      <td>2025-06-08</td>\n",
              "      <td>6014.205838</td>\n",
              "      <td>0.197482</td>\n",
              "      <td>0.536615</td>\n",
              "      <td>0.184140</td>\n",
              "      <td>0.035530</td>\n",
              "      <td>0.196871</td>\n",
              "      <td>61258.024739</td>\n",
              "      <td>48449.102347</td>\n",
              "      <td>63222.581574</td>\n",
              "      <td>5875.828796</td>\n",
              "      <td>31326.684255</td>\n",
              "      <td>8.702046</td>\n",
              "      <td>11.022866</td>\n",
              "      <td>10.788290</td>\n",
              "      <td>11.054433</td>\n",
              "      <td>8.678773</td>\n",
              "      <td>10.352257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1307</th>\n",
              "      <td>10</td>\n",
              "      <td>2025-06-15</td>\n",
              "      <td>6722.581824</td>\n",
              "      <td>0.860886</td>\n",
              "      <td>0.828648</td>\n",
              "      <td>0.107689</td>\n",
              "      <td>0.559509</td>\n",
              "      <td>0.242228</td>\n",
              "      <td>12548.255828</td>\n",
              "      <td>43434.769555</td>\n",
              "      <td>32306.555176</td>\n",
              "      <td>17573.507161</td>\n",
              "      <td>63274.505705</td>\n",
              "      <td>8.813376</td>\n",
              "      <td>9.437417</td>\n",
              "      <td>10.679039</td>\n",
              "      <td>10.383056</td>\n",
              "      <td>9.774205</td>\n",
              "      <td>11.055254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1308</th>\n",
              "      <td>10</td>\n",
              "      <td>2025-06-22</td>\n",
              "      <td>620.502304</td>\n",
              "      <td>0.823068</td>\n",
              "      <td>0.138851</td>\n",
              "      <td>0.437612</td>\n",
              "      <td>0.721622</td>\n",
              "      <td>0.841589</td>\n",
              "      <td>16337.065503</td>\n",
              "      <td>19778.196930</td>\n",
              "      <td>77309.609239</td>\n",
              "      <td>78198.384217</td>\n",
              "      <td>37931.698244</td>\n",
              "      <td>6.432140</td>\n",
              "      <td>9.701253</td>\n",
              "      <td>9.892386</td>\n",
              "      <td>11.255586</td>\n",
              "      <td>11.267017</td>\n",
              "      <td>10.543569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1309</th>\n",
              "      <td>10</td>\n",
              "      <td>2025-06-29</td>\n",
              "      <td>9460.646921</td>\n",
              "      <td>0.902150</td>\n",
              "      <td>0.909937</td>\n",
              "      <td>0.564828</td>\n",
              "      <td>0.279978</td>\n",
              "      <td>0.741193</td>\n",
              "      <td>30381.323704</td>\n",
              "      <td>89624.875624</td>\n",
              "      <td>18917.702075</td>\n",
              "      <td>38218.391669</td>\n",
              "      <td>69740.722729</td>\n",
              "      <td>9.155002</td>\n",
              "      <td>10.321616</td>\n",
              "      <td>11.403399</td>\n",
              "      <td>9.847906</td>\n",
              "      <td>10.551098</td>\n",
              "      <td>11.152554</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1310 rows × 19 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      store_id week_start           GC  digital_promo_1  digital_promo_2  \\\n",
              "0            1 2023-01-01  7762.164881         0.392485         0.877477   \n",
              "1            1 2023-01-08  4444.896554         0.923900         0.959596   \n",
              "2            1 2023-01-15  8600.119407         0.578978         0.228336   \n",
              "3            1 2023-01-22  7003.943488         0.004619         0.931824   \n",
              "4            1 2023-01-29  1032.355744         0.038542         0.571213   \n",
              "...        ...        ...          ...              ...              ...   \n",
              "1305        10 2025-06-01  9282.298110         0.494260         0.560057   \n",
              "1306        10 2025-06-08  6014.205838         0.197482         0.536615   \n",
              "1307        10 2025-06-15  6722.581824         0.860886         0.828648   \n",
              "1308        10 2025-06-22   620.502304         0.823068         0.138851   \n",
              "1309        10 2025-06-29  9460.646921         0.902150         0.909937   \n",
              "\n",
              "      digital_promo_3  digital_promo_4  digital_promo_5       media_1  \\\n",
              "0            0.531428         0.138708         0.863492  61326.061549   \n",
              "1            0.586109         0.293273         0.240285   3988.161026   \n",
              "2            0.510166         0.305943         0.839874  12813.036067   \n",
              "3            0.036025         0.261298         0.078545  50045.750273   \n",
              "4            0.497091         0.460761         0.159928  74659.258456   \n",
              "...               ...              ...              ...           ...   \n",
              "1305         0.626922         0.845786         0.792066  39787.334767   \n",
              "1306         0.184140         0.035530         0.196871  61258.024739   \n",
              "1307         0.107689         0.559509         0.242228  12548.255828   \n",
              "1308         0.437612         0.721622         0.841589  16337.065503   \n",
              "1309         0.564828         0.279978         0.741193  30381.323704   \n",
              "\n",
              "           media_2       media_3       media_4       media_5    log_GC  \\\n",
              "0     17201.343184  97417.410477  71648.944633   3607.031881  8.957145   \n",
              "1     67742.162031   5269.544085  62586.983836  16846.864943  8.399737   \n",
              "2      2574.355308  79595.063348  65263.859444  41218.643405  9.059648   \n",
              "3     16101.731418  64607.084138  17677.173940  71819.519391  8.854371   \n",
              "4     90270.550139  26217.482191  66950.482733  36412.155677  6.940567   \n",
              "...            ...           ...           ...           ...       ...   \n",
              "1305   6773.695983    913.135423  53610.359616  88195.150862  9.135972   \n",
              "1306  48449.102347  63222.581574   5875.828796  31326.684255  8.702046   \n",
              "1307  43434.769555  32306.555176  17573.507161  63274.505705  8.813376   \n",
              "1308  19778.196930  77309.609239  78198.384217  37931.698244  6.432140   \n",
              "1309  89624.875624  18917.702075  38218.391669  69740.722729  9.155002   \n",
              "\n",
              "      log_media_1  log_media_2  log_media_3  log_media_4  log_media_5  \n",
              "0       11.023976     9.752801    11.486770    11.179548     8.190918  \n",
              "1        8.291336    11.123479     8.569889    11.044329     9.731979  \n",
              "2        9.458296     7.853743    11.284720    11.086209    10.626670  \n",
              "3       10.820713     9.686744    11.076095     9.780086    11.181925  \n",
              "4       11.220703    11.410578    10.174220    11.111723    10.502685  \n",
              "...           ...          ...          ...          ...          ...  \n",
              "1305    10.591329     8.820950     6.817979    10.889516    11.387319  \n",
              "1306    11.022866    10.788290    11.054433     8.678773    10.352257  \n",
              "1307     9.437417    10.679039    10.383056     9.774205    11.055254  \n",
              "1308     9.701253     9.892386    11.255586    11.267017    10.543569  \n",
              "1309    10.321616    11.403399     9.847906    10.551098    11.152554  \n",
              "\n",
              "[1310 rows x 19 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) Configuration objects (easy to extend)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TimeSplit:\n",
        "    time_split_id: str\n",
        "    train_start: str\n",
        "    train_end: str\n",
        "    test_start: str\n",
        "    test_end: str\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class RunConfig:\n",
        "    # \"fixed\" knobs you said exist\n",
        "    target_node: str              # \"GC\" or \"AC\"\n",
        "    panel_control: str            # \"FE\" / \"Mundlak\" / \"Bayesian\" (placeholder for now)\n",
        "    algorithm: str                # \"OLS\" / \"Ridge\" / \"ElasticNet\"\n",
        "    alpha: float = 0.0\n",
        "    l1_ratio: float = 0.0\n",
        "\n",
        "    # experiment dimensions\n",
        "    feature_block_set_id: str = \"\"\n",
        "    features: Tuple[str, ...] = tuple()\n",
        "    time_split: TimeSplit = None\n",
        "    seed: int = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2) MLflow setup (experiment + common tags)\n",
        "\n",
        "Databricks tip: use a workspace path like \"/Users/<you>/RGA/Regressions\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def setup_mlflow(experiment_name: str, common_tags: Dict[str, str]) -> None:\n",
        "#     mlflow.set_experiment(experiment_name)\n",
        "#     mlflow.set_tags(common_tags)\n",
        "\n",
        "def setup_mlflow(experiment_name: str) -> None:\n",
        "    mlflow.set_experiment(experiment_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3) Search strategy component (swap later)\n",
        "\n",
        "This yields a stream of RunConfig objects. Keep it dumb/simple now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3) Feature subset sampler (K-per-block)\n",
        "# =========================\n",
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "\n",
        "def sample_k_per_block(\n",
        "    feature_blocks: Dict[str, List[str]],\n",
        "    k_per_block: Dict[str, int],\n",
        "    n_samples: int,\n",
        "    seed: int = 42,\n",
        "    allow_all_if_small: bool = True,\n",
        ") -> List[Tuple[str, Tuple[str, ...]]]:\n",
        "    \"\"\"\n",
        "    Returns list of (feature_set_id, features_tuple).\n",
        "    Each sample picks K features from EACH block and unions them.\n",
        "\n",
        "    Example k_per_block: {\"promo\":3, \"media\":3, \"ops\":2}\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    blocks = list(feature_blocks.keys())\n",
        "    if set(k_per_block.keys()) != set(blocks):\n",
        "        missing = set(blocks) - set(k_per_block.keys())\n",
        "        extra = set(k_per_block.keys()) - set(blocks)\n",
        "        raise ValueError(f\"k_per_block keys must match feature_blocks keys. missing={missing}, extra={extra}\")\n",
        "\n",
        "    results = []\n",
        "    seen = set()\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        chosen = []\n",
        "        for b in blocks:\n",
        "            feats = feature_blocks[b]\n",
        "            k = k_per_block[b]\n",
        "\n",
        "            if len(feats) < k:\n",
        "                if allow_all_if_small:\n",
        "                    pick = list(feats)\n",
        "                else:\n",
        "                    raise ValueError(f\"Block '{b}' has only {len(feats)} features but k={k}.\")\n",
        "            else:\n",
        "                pick = rng.choice(feats, size=k, replace=False).tolist()\n",
        "\n",
        "            chosen.extend(pick)\n",
        "\n",
        "        chosen_sorted = tuple(sorted(set(chosen)))\n",
        "        if chosen_sorted in seen:\n",
        "            continue\n",
        "        seen.add(chosen_sorted)\n",
        "\n",
        "        fs_id = f\"kperblock__{i:04d}\"\n",
        "        results.append((fs_id, chosen_sorted))\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def generate_feature_sets(feature_blocks: Dict[str, List[str]]) -> Dict[str, Tuple[str, ...]]:\n",
        "#     \"\"\"\n",
        "#     Example: feature_blocks = {\"promo\":[...], \"media\":[...], \"ops\":[...]}\n",
        "#     Return a dict of feature_block_set_id -> tuple(features)\n",
        "#     \"\"\"\n",
        "#     # Minimal example: each single block + all blocks\n",
        "#     out = {}\n",
        "#     for block, feats in feature_blocks.items():\n",
        "#         out[f\"block__{block}\"] = tuple(feats)\n",
        "#     all_feats = tuple(sorted({f for feats in feature_blocks.values() for f in feats}))\n",
        "#     out[\"block__ALL\"] = all_feats\n",
        "#     return out\n",
        "\n",
        "# def search_space(\n",
        "#     target_node: str,\n",
        "#     panel_controls: List[str],\n",
        "#     algorithms: List[Dict[str, Any]],\n",
        "#     time_splits: List[TimeSplit],\n",
        "#     feature_sets: Dict[str, Tuple[str, ...]],\n",
        "#     seed: int = 42,\n",
        "# ) -> Iterator[RunConfig]:\n",
        "#     for pc in panel_controls:\n",
        "#         for algo in algorithms:\n",
        "#             for ts in time_splits:\n",
        "#                 for fs_id, feats in feature_sets.items():\n",
        "#                     yield RunConfig(\n",
        "#                         target_node=target_node,\n",
        "#                         panel_control=pc,\n",
        "#                         algorithm=algo[\"name\"],\n",
        "#                         alpha=float(algo.get(\"alpha\", 0.0)),\n",
        "#                         l1_ratio=float(algo.get(\"l1_ratio\", 0.0)),\n",
        "#                         feature_block_set_id=fs_id,\n",
        "#                         features=feats,\n",
        "#                         time_split=ts,\n",
        "#                         seed=seed,\n",
        "#                     )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterator, Dict, Any, List, Tuple\n",
        "\n",
        "def search_space_with_feature_sets(\n",
        "    target_node: str,\n",
        "    panel_controls: List[str],\n",
        "    algorithms: List[Dict[str, Any]],\n",
        "    time_splits: List[TimeSplit],\n",
        "    feature_sets: List[Tuple[str, Tuple[str, ...]]],\n",
        "    seed: int = 42,\n",
        ") -> Iterator[RunConfig]:\n",
        "    for pc in panel_controls:\n",
        "        for algo in algorithms:\n",
        "            for ts in time_splits:\n",
        "                for fs_id, feats in feature_sets:\n",
        "                    yield RunConfig(\n",
        "                        target_node=target_node,\n",
        "                        panel_control=pc,\n",
        "                        algorithm=algo[\"name\"],\n",
        "                        alpha=float(algo.get(\"alpha\", 0.0)),\n",
        "                        l1_ratio=float(algo.get(\"l1_ratio\", 0.0)),\n",
        "                        feature_block_set_id=fs_id,\n",
        "                        features=feats,\n",
        "                        time_split=ts,\n",
        "                        seed=seed,\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4) Model factory component (swap later)\n",
        "\n",
        "This is where FE/Mundlak/Bayesian wrappers will go. For now it’s vanilla sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Callable\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class FitResult:\n",
        "    model: object\n",
        "    coef_df: pd.DataFrame\n",
        "    predict_fn: Callable[[pd.DataFrame], np.ndarray]  # predict on any df\n",
        "\n",
        "def standardize_coef_df(feature_index, coef, t_stat=None, p_value=None) -> pd.DataFrame:\n",
        "    df = pd.DataFrame({\n",
        "        \"feature\": list(feature_index),\n",
        "        \"coef\": np.asarray(coef, dtype=float),\n",
        "        \"t_stat\": np.asarray(t_stat, dtype=float) if t_stat is not None else np.nan,\n",
        "        \"p_value\": np.asarray(p_value, dtype=float) if p_value is not None else np.nan,\n",
        "    })\n",
        "    df[\"abs_coef\"] = df[\"coef\"].abs()\n",
        "    df[\"sign\"] = np.sign(df[\"coef\"]).astype(int)\n",
        "    df = df.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
        "    df[\"rank_abscoef\"] = np.arange(1, len(df) + 1)\n",
        "    df[\"is_significant_05\"] = df[\"p_value\"].apply(lambda x: (x < 0.05) if pd.notna(x) else False)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from linearmodels.panel import PanelOLS\n",
        "import pandas as pd\n",
        "\n",
        "def fit_fe_linearmodels(\n",
        "    df_train: pd.DataFrame,\n",
        "    y_col: str,\n",
        "    x_cols: List[str],\n",
        "    entity_col: str = \"store_id\",\n",
        "    time_col: str = \"week_start\",\n",
        "    time_effects: bool = False,\n",
        ") -> FitResult:\n",
        "    df_tr = df_train.copy()\n",
        "    df_tr[time_col] = pd.to_datetime(df_tr[time_col])\n",
        "    df_tr = df_tr.set_index([entity_col, time_col]).sort_index()\n",
        "\n",
        "    y = df_tr[y_col].astype(float)\n",
        "    X = df_tr[x_cols].astype(float)\n",
        "\n",
        "    mod = PanelOLS(y, X, entity_effects=True, time_effects=time_effects)\n",
        "    res = mod.fit(cov_type=\"clustered\", cluster_entity=True)\n",
        "\n",
        "    coef_df = standardize_coef_df(\n",
        "        res.params.index,\n",
        "        res.params.values,\n",
        "        t_stat=res.tstats.reindex(res.params.index).values,\n",
        "        p_value=res.pvalues.reindex(res.params.index).values,\n",
        "    )\n",
        "\n",
        "    def predict_fn(df_any: pd.DataFrame) -> np.ndarray:\n",
        "        df_te = df_any.copy()\n",
        "        df_te[time_col] = pd.to_datetime(df_te[time_col])\n",
        "        df_te = df_te.set_index([entity_col, time_col]).sort_index()\n",
        "        X_te = df_te[x_cols].astype(float)\n",
        "        pred = res.predict(exog=X_te)\n",
        "        # res.predict returns a DataFrame/Series aligned to index\n",
        "        return np.asarray(pred).ravel()\n",
        "\n",
        "    return FitResult(model=res, coef_df=coef_df, predict_fn=predict_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Callable, Optional, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.mixed_linear_model import MixedLM\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HLMFitResult:\n",
        "    model: object                       # fitted statsmodels result\n",
        "    coef_df: pd.DataFrame               # fixed-effect coefficients + inference\n",
        "    predict_fn: Callable[[pd.DataFrame], np.ndarray]   # prediction function\n",
        "\n",
        "\n",
        "def fit_hlm_mixedlm(\n",
        "    df_train: pd.DataFrame,\n",
        "    y_col: str,\n",
        "    x_cols: List[str],\n",
        "    group_col: str = \"store_id\",\n",
        "    add_intercept: bool = True,\n",
        "    random_slopes: Optional[List[str]] = None,\n",
        "    reml: bool = True,\n",
        "    maxiter: int = 200,\n",
        "    method: str = \"lbfgs\",\n",
        ") -> HLMFitResult:\n",
        "    \"\"\"\n",
        "    Hierarchical Linear Model (Mixed Effects) via statsmodels MixedLM.\n",
        "\n",
        "    Default: random intercept by store_id\n",
        "      y ~ X (fixed effects) + (1 | store)\n",
        "\n",
        "    Optionally: random slopes for selected features (small list recommended)\n",
        "      y ~ X + (1 + z1 + z2 | store)\n",
        "\n",
        "    Returns:\n",
        "      - fitted model result\n",
        "      - coef_df: fixed-effect table with std err, z, pvalue\n",
        "      - predict_fn: predicts using fixed effects + estimated random effects when available\n",
        "    \"\"\"\n",
        "    if random_slopes is None:\n",
        "        random_slopes = []\n",
        "\n",
        "    # Basic column checks\n",
        "    needed = [y_col, group_col] + x_cols + list(random_slopes)\n",
        "    missing = [c for c in needed if c not in df_train.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns for MixedLM: {missing}\")\n",
        "\n",
        "    df = df_train.copy()\n",
        "\n",
        "    # Build fixed effects design matrix\n",
        "    X = df[x_cols].astype(float)\n",
        "    if add_intercept:\n",
        "        X = sm.add_constant(X, has_constant=\"add\")\n",
        "\n",
        "    y = df[y_col].astype(float)\n",
        "    groups = df[group_col]\n",
        "\n",
        "    # Build random effects design matrix\n",
        "    # - If empty: random intercept only\n",
        "    # - If slopes: include intercept + selected slope columns\n",
        "    if len(random_slopes) == 0:\n",
        "        # random intercept only -> a single column of ones\n",
        "        Z = np.ones((len(df), 1), dtype=float)\n",
        "        re_names = [\"re_intercept\"]\n",
        "    else:\n",
        "        Z = df[random_slopes].astype(float).copy()\n",
        "        # include random intercept as well\n",
        "        Z.insert(0, \"re_intercept\", 1.0)\n",
        "        re_names = list(Z.columns)\n",
        "        Z = Z.to_numpy()\n",
        "\n",
        "    # Fit MixedLM\n",
        "    model = MixedLM(endog=y, exog=X, groups=groups, exog_re=Z)\n",
        "    res = model.fit(reml=reml, method=method, maxiter=maxiter, disp=False)\n",
        "\n",
        "    # Fixed-effect coefficient table\n",
        "    # res.fe_params / res.bse_fe are fixed effects only\n",
        "    fe_params = res.fe_params\n",
        "    bse_fe = res.bse_fe\n",
        "\n",
        "    zvals = fe_params / bse_fe\n",
        "    # two-sided p-values (normal approx)\n",
        "    pvals = 2 * (1 - sm.stats.norm.cdf(np.abs(zvals)))\n",
        "\n",
        "    coef_df = pd.DataFrame({\n",
        "        \"feature\": fe_params.index,\n",
        "        \"coef\": fe_params.values,\n",
        "        \"std_err\": bse_fe.values,\n",
        "        \"z_stat\": zvals.values,\n",
        "        \"p_value\": pvals.values,\n",
        "    })\n",
        "\n",
        "    # Add sign / rank\n",
        "    coef_df[\"abs_coef\"] = coef_df[\"coef\"].abs()\n",
        "    coef_df[\"sign\"] = np.sign(coef_df[\"coef\"]).astype(int)\n",
        "    coef_df = coef_df.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
        "    coef_df[\"rank_abscoef\"] = np.arange(1, len(coef_df) + 1)\n",
        "    coef_df[\"is_significant_05\"] = coef_df[\"p_value\"] < 0.05\n",
        "\n",
        "    # Prediction function\n",
        "    # Uses fixed effects always; uses random effects for known groups if available\n",
        "    def predict_fn(df_any: pd.DataFrame) -> np.ndarray:\n",
        "        dfp = df_any.copy()\n",
        "        Xp = dfp[x_cols].astype(float)\n",
        "        if add_intercept:\n",
        "            Xp = sm.add_constant(Xp, has_constant=\"add\")\n",
        "\n",
        "        # Fixed part\n",
        "        yhat = np.asarray(Xp @ res.fe_params, dtype=float)\n",
        "\n",
        "        # Random part (if group seen in training)\n",
        "        # res.random_effects: dict(group -> array of RE coefficients)\n",
        "        if hasattr(res, \"random_effects\") and res.random_effects is not None:\n",
        "            g = dfp[group_col].values\n",
        "            for i in range(len(dfp)):\n",
        "                gi = g[i]\n",
        "                if gi in res.random_effects:\n",
        "                    b = np.asarray(res.random_effects[gi]).ravel()  # (q,)\n",
        "                    if len(random_slopes) == 0:\n",
        "                        # random intercept only\n",
        "                        yhat[i] += b[0]\n",
        "                    else:\n",
        "                        # intercept + slopes\n",
        "                        # build row vector [1, z1, z2, ...]\n",
        "                        zrow = [1.0] + [float(dfp.iloc[i][c]) for c in random_slopes]\n",
        "                        yhat[i] += float(np.dot(b, np.asarray(zrow, dtype=float)))\n",
        "\n",
        "        return yhat\n",
        "\n",
        "    return HLMFitResult(model=res, coef_df=coef_df, predict_fn=predict_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_hlm_as_fitresult(\n",
        "    df_train: pd.DataFrame,\n",
        "    y_col: str,\n",
        "    x_cols: List[str],\n",
        "    group_col: str = \"store_id\",\n",
        "    add_intercept: bool = True,\n",
        "    random_slopes: Optional[List[str]] = None,\n",
        "    reml: bool = True,\n",
        "    maxiter: int = 200,\n",
        "    method: str = \"lbfgs\",\n",
        ") -> Tuple[FitResult, str]:\n",
        "    \"\"\"\n",
        "    Wrap your existing fit_hlm_mixedlm() so the rest of the pipeline\n",
        "    can treat it like any other backend (FitResult).\n",
        "    Returns:\n",
        "      - FitResult (model, coef_df, predict_fn)\n",
        "      - summary_text (for artifact logging)\n",
        "    \"\"\"\n",
        "    hlm_res = fit_hlm_mixedlm(\n",
        "        df_train=df_train,\n",
        "        y_col=y_col,\n",
        "        x_cols=x_cols,\n",
        "        group_col=group_col,\n",
        "        add_intercept=add_intercept,\n",
        "        random_slopes=random_slopes,\n",
        "        reml=reml,\n",
        "        maxiter=maxiter,\n",
        "        method=method,\n",
        "    )\n",
        "    summary_text = hlm_res.model.summary().as_text()\n",
        "    fit_res = FitResult(model=hlm_res.model, coef_df=hlm_res.coef_df, predict_fn=hlm_res.predict_fn)\n",
        "    return fit_res, summary_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random intercept only (recommended default)\n",
        "fit_res = fit_hlm_mixedlm(\n",
        "    df_train=train_df,\n",
        "    y_col=\"log_GC\",\n",
        "    x_cols=list(cfg.features),\n",
        "    group_col=\"store_id\",\n",
        "    random_slopes=None\n",
        ")\n",
        "\n",
        "cdf = fit_res.coef_df\n",
        "pred_test = fit_res.predict_fn(test_df)\n",
        "\n",
        "# Only pick 1–3 “key drivers” for random slopes, otherwise it can get slow/unstable\n",
        "fit_res = fit_hlm_mixedlm(\n",
        "    df_train=train_df,\n",
        "    y_col=\"log_GC\",\n",
        "    x_cols=list(cfg.features),\n",
        "    group_col=\"store_id\",\n",
        "    random_slopes=[\"avg_price\", \"promo_depth\"]  # small list\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from linearmodels.panel import RandomEffects\n",
        "import statsmodels.api as sm\n",
        "\n",
        "def fit_mundlak_linearmodels(\n",
        "    df_train: pd.DataFrame,\n",
        "    y_col: str,\n",
        "    x_cols: List[str],\n",
        "    entity_col: str = \"store_id\",\n",
        "    time_col: str = \"week_start\",\n",
        "    include_intercept: bool = True,\n",
        ") -> FitResult:\n",
        "    df_tr = df_train.copy()\n",
        "    df_tr[time_col] = pd.to_datetime(df_tr[time_col])\n",
        "\n",
        "    # Compute entity means on TRAIN ONLY (no leakage)\n",
        "    means_by_entity = df_tr.groupby(entity_col)[x_cols].mean()\n",
        "\n",
        "    def make_X_aug(df_any: pd.DataFrame) -> pd.DataFrame:\n",
        "        df_any = df_any.copy()\n",
        "        # join train means to any df (unseen entities -> NaN; you can choose how to handle later)\n",
        "        m = df_any[[entity_col]].merge(\n",
        "            means_by_entity.reset_index(),\n",
        "            on=entity_col,\n",
        "            how=\"left\",\n",
        "        )\n",
        "        mean_cols = {c: f\"{c}__mean_by_entity\" for c in x_cols}\n",
        "        for c in x_cols:\n",
        "            df_any[mean_cols[c]] = m[c].values\n",
        "        X = df_any[x_cols].astype(float)\n",
        "        X_means = df_any[[mean_cols[c] for c in x_cols]].astype(float)\n",
        "        X_aug = pd.concat([X, X_means], axis=1)\n",
        "        if include_intercept:\n",
        "            X_aug = sm.add_constant(X_aug, has_constant=\"add\")\n",
        "        return X_aug\n",
        "\n",
        "    # Panel index\n",
        "    df_tr = df_tr.set_index([entity_col, time_col]).sort_index()\n",
        "    y = df_tr[y_col].astype(float)\n",
        "\n",
        "    # Need X_aug aligned with same index\n",
        "    df_tr_reset = df_tr.reset_index()\n",
        "    X_aug = make_X_aug(df_tr_reset)\n",
        "    X_aug.index = df_tr.index  # align to panel index\n",
        "\n",
        "    mod = RandomEffects(y, X_aug)\n",
        "    res = mod.fit(cov_type=\"clustered\", cluster_entity=True)\n",
        "\n",
        "    coef_df = standardize_coef_df(\n",
        "        res.params.index,\n",
        "        res.params.values,\n",
        "        t_stat=res.tstats.reindex(res.params.index).values,\n",
        "        p_value=res.pvalues.reindex(res.params.index).values,\n",
        "    )\n",
        "\n",
        "    def predict_fn(df_any: pd.DataFrame) -> np.ndarray:\n",
        "        df_te = df_any.copy()\n",
        "        df_te[time_col] = pd.to_datetime(df_te[time_col])\n",
        "        df_te = df_te.set_index([entity_col, time_col]).sort_index()\n",
        "\n",
        "        df_te_reset = df_te.reset_index()\n",
        "        X_te_aug = make_X_aug(df_te_reset)\n",
        "        X_te_aug.index = df_te.index\n",
        "\n",
        "        pred = res.predict(exog=X_te_aug)\n",
        "        return np.asarray(pred).ravel()\n",
        "\n",
        "    return FitResult(model=res, coef_df=coef_df, predict_fn=predict_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Callable, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def fit_bayesian_hierarchical_pymc(\n",
        "    df_train: pd.DataFrame,\n",
        "    y_col: str,\n",
        "    x_cols: List[str],\n",
        "    group_col: str = \"store_id\",\n",
        "    standardize_X: bool = True,\n",
        "    inference: str = \"advi\",          # \"advi\" (fast) or \"nuts\" (slower, more accurate)\n",
        "    advi_steps: int = 20000,\n",
        "    nuts_draws: int = 1000,\n",
        "    nuts_tune: int = 1000,\n",
        "    random_seed: int = 42,\n",
        "):\n",
        "    \"\"\"\n",
        "    Bayesian hierarchical linear model with random intercept by group (store).\n",
        "    Returns:\n",
        "      model, idata, coef_df, predict_fn\n",
        "    \"\"\"\n",
        "    import pymc as pm\n",
        "    import arviz as az\n",
        "\n",
        "    df = df_train.copy()\n",
        "\n",
        "    # --- Encode group as consecutive integers ---\n",
        "    groups = pd.Categorical(df[group_col])\n",
        "    g_idx = groups.codes\n",
        "    n_groups = len(groups.categories)\n",
        "\n",
        "    # --- Build design matrix ---\n",
        "    X = df[x_cols].astype(float).to_numpy()\n",
        "    y = df[y_col].astype(float).to_numpy()\n",
        "\n",
        "    x_means = X.mean(axis=0)\n",
        "    x_stds = X.std(axis=0, ddof=0)\n",
        "    x_stds = np.where(x_stds == 0, 1.0, x_stds)\n",
        "\n",
        "    if standardize_X:\n",
        "        Xs = (X - x_means) / x_stds\n",
        "    else:\n",
        "        Xs = X\n",
        "\n",
        "    with pm.Model() as model:\n",
        "        # Hyperpriors for random intercepts\n",
        "        sigma_a = pm.HalfNormal(\"sigma_a\", sigma=1.0)\n",
        "        a = pm.Normal(\"a\", mu=0.0, sigma=sigma_a, shape=n_groups)  # group intercepts\n",
        "\n",
        "        # Fixed effects priors\n",
        "        beta = pm.Normal(\"beta\", mu=0.0, sigma=1.0, shape=Xs.shape[1])\n",
        "\n",
        "        # Noise\n",
        "        sigma = pm.HalfNormal(\"sigma\", sigma=1.0)\n",
        "\n",
        "        mu = a[g_idx] + pm.math.dot(Xs, beta)\n",
        "        pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y)\n",
        "\n",
        "        if inference.lower() == \"advi\":\n",
        "            approx = pm.fit(\n",
        "                n=advi_steps,\n",
        "                method=\"advi\",\n",
        "                random_seed=random_seed,\n",
        "            )\n",
        "            idata = approx.sample(1000)\n",
        "        elif inference.lower() == \"nuts\":\n",
        "            idata = pm.sample(\n",
        "                draws=nuts_draws,\n",
        "                tune=nuts_tune,\n",
        "                chains=2,\n",
        "                target_accept=0.9,\n",
        "                random_seed=random_seed,\n",
        "                progressbar=True,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"inference must be 'advi' or 'nuts'\")\n",
        "\n",
        "    # --- Coefficient summary ---\n",
        "    # beta is on standardized scale if standardize_X=True\n",
        "    beta_samples = idata.posterior[\"beta\"].stack(sample=(\"chain\", \"draw\")).values  # [p, samples]\n",
        "    beta_mean = beta_samples.mean(axis=1)\n",
        "    beta_sd = beta_samples.std(axis=1, ddof=0)\n",
        "\n",
        "    hdi = az.hdi(idata.posterior[\"beta\"], hdi_prob=0.94).to_array().values  # shape [2, p]\n",
        "    hdi_low, hdi_high = hdi[0, :], hdi[1, :]\n",
        "\n",
        "    p_gt_0 = (beta_samples > 0).mean(axis=1)\n",
        "    p_lt_0 = (beta_samples < 0).mean(axis=1)\n",
        "\n",
        "    coef_df = pd.DataFrame({\n",
        "        \"feature\": x_cols,\n",
        "        \"coef_post_mean\": beta_mean,\n",
        "        \"coef_post_sd\": beta_sd,\n",
        "        \"hdi94_low\": hdi_low,\n",
        "        \"hdi94_high\": hdi_high,\n",
        "        \"p_gt_0\": p_gt_0,\n",
        "        \"p_lt_0\": p_lt_0,\n",
        "        \"sign_post\": np.sign(beta_mean).astype(int),\n",
        "    })\n",
        "\n",
        "    # Optional: convert back to original scale if you standardized X\n",
        "    if standardize_X:\n",
        "        coef_df[\"coef_post_mean_unscaled\"] = coef_df[\"coef_post_mean\"] / x_stds\n",
        "        coef_df[\"coef_post_sd_unscaled\"] = coef_df[\"coef_post_sd\"] / x_stds\n",
        "    else:\n",
        "        coef_df[\"coef_post_mean_unscaled\"] = coef_df[\"coef_post_mean\"]\n",
        "        coef_df[\"coef_post_sd_unscaled\"] = coef_df[\"coef_post_sd\"]\n",
        "\n",
        "    coef_df[\"abs_coef\"] = coef_df[\"coef_post_mean_unscaled\"].abs()\n",
        "    coef_df = coef_df.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
        "    coef_df[\"rank_abscoef\"] = np.arange(1, len(coef_df) + 1)\n",
        "\n",
        "    # --- Predict function ---\n",
        "    def predict_fn(df_any: pd.DataFrame) -> np.ndarray:\n",
        "        dfp = df_any.copy()\n",
        "        # map groups\n",
        "        gcat = pd.Categorical(dfp[group_col], categories=groups.categories)\n",
        "        g = gcat.codes\n",
        "        Xp = dfp[x_cols].astype(float).to_numpy()\n",
        "        Xps = (Xp - x_means) / x_stds if standardize_X else Xp\n",
        "\n",
        "        # posterior mean prediction\n",
        "        a_mean = idata.posterior[\"a\"].mean(dim=(\"chain\", \"draw\")).values\n",
        "        beta_mean_local = idata.posterior[\"beta\"].mean(dim=(\"chain\", \"draw\")).values\n",
        "        mu_hat = a_mean[g] + Xps @ beta_mean_local\n",
        "        return np.asarray(mu_hat, dtype=float)\n",
        "\n",
        "    return model, idata, coef_df, predict_fn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
        "\n",
        "def fit_sklearn_pooled(\n",
        "    df_train: pd.DataFrame,\n",
        "    y_col: str,\n",
        "    x_cols: List[str],\n",
        "    algorithm: str,\n",
        "    alpha: float,\n",
        "    l1_ratio: float,\n",
        "    seed: int = 42,\n",
        ") -> FitResult:\n",
        "    X = df_train[x_cols].astype(float)\n",
        "    y = df_train[y_col].astype(float)\n",
        "\n",
        "    if algorithm == \"OLS\":\n",
        "        model = LinearRegression()\n",
        "    elif algorithm == \"Ridge\":\n",
        "        model = Ridge(alpha=alpha, random_state=seed)\n",
        "    elif algorithm == \"ElasticNet\":\n",
        "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=seed, max_iter=10000)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported algorithm for sklearn pooled: {algorithm}\")\n",
        "\n",
        "    model.fit(X, y)\n",
        "    coef_df = standardize_coef_df(x_cols, model.coef_, t_stat=None, p_value=None)\n",
        "\n",
        "    def predict_fn(df_any: pd.DataFrame) -> np.ndarray:\n",
        "        return model.predict(df_any[x_cols].astype(float))\n",
        "\n",
        "    return FitResult(model=model, coef_df=coef_df, predict_fn=predict_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_with_backend(\n",
        "    df_train: pd.DataFrame,\n",
        "    y_col: str,\n",
        "    x_cols: List[str],\n",
        "    cfg: RunConfig,\n",
        ") -> FitResult:\n",
        "    pc = cfg.panel_control.lower().strip()\n",
        "\n",
        "    # If you asked for FE/Mundlak, we apply panel methods only for OLS-like estimation\n",
        "    if pc == \"fe\" and cfg.algorithm == \"OLS\":\n",
        "        return fit_fe_linearmodels(df_train, y_col, x_cols, time_effects=False)\n",
        "\n",
        "    if pc == \"mundlak\" and cfg.algorithm == \"OLS\":\n",
        "        return fit_mundlak_linearmodels(df_train, y_col, x_cols, include_intercept=True)\n",
        "\n",
        "    if pc == \"hlm\" and cfg.algorithm == \"OLS\":\n",
        "        fit_res, summary_text = fit_hlm_as_fitresult(\n",
        "            df_train=df_train,\n",
        "            y_col=y_col,\n",
        "            x_cols=x_cols,\n",
        "            group_col=\"store_id\",\n",
        "            random_slopes=None,   # start simple; you can parameterize later\n",
        "            reml=True,\n",
        "            maxiter=200,\n",
        "            method=\"lbfgs\",\n",
        "        )\n",
        "        return fit_res, summary_text\n",
        "    \n",
        "    # Otherwise fallback pooled sklearn (Ridge / ElasticNet / pooled OLS)\n",
        "    return fit_sklearn_pooled(\n",
        "        df_train=df_train,\n",
        "        y_col=y_col,\n",
        "        x_cols=x_cols,\n",
        "        algorithm=cfg.algorithm,\n",
        "        alpha=cfg.alpha,\n",
        "        l1_ratio=cfg.l1_ratio,\n",
        "        seed=cfg.seed,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_summary_text(fit_model: object) -> str:\n",
        "    \"\"\"\n",
        "    Return a readable text summary for different fitted model types.\n",
        "    \"\"\"\n",
        "    # statsmodels (OLS / MixedLM results)\n",
        "    if hasattr(fit_model, \"summary\"):\n",
        "        try:\n",
        "            s = fit_model.summary()\n",
        "            # statsmodels summary object\n",
        "            if hasattr(s, \"as_text\"):\n",
        "                return s.as_text()\n",
        "            return str(s)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # linearmodels (PanelOLS / RandomEffects results)\n",
        "    # linearmodels results usually have .summary as a Summary instance printable to string\n",
        "    if hasattr(fit_model, \"summary\"):\n",
        "        try:\n",
        "            return str(fit_model.summary)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # sklearn models (Ridge / ElasticNet / LinearRegression)\n",
        "    # no built-in summary; log a simple snapshot\n",
        "    params = {}\n",
        "    for attr in [\"coef_\", \"intercept_\", \"alpha\", \"l1_ratio\"]:\n",
        "        if hasattr(fit_model, attr):\n",
        "            val = getattr(fit_model, attr)\n",
        "            if isinstance(val, np.ndarray):\n",
        "                params[attr] = f\"ndarray shape={val.shape}\"\n",
        "            else:\n",
        "                params[attr] = val\n",
        "\n",
        "    return \"Model summary not available for this model type.\\n\" + str(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # -------------------------\n",
        "# # 4D) Simple factory\n",
        "# # -------------------------\n",
        "# def make_fit_backend(panel_control: str) -> FitBackend:\n",
        "#     panel_control = panel_control.strip().lower()\n",
        "#     if panel_control == \"fe\":\n",
        "#         # set time_effects=True if you want week FE as well (often costly but sometimes useful)\n",
        "#         return FixedEffectsBackend(entity_effects=True, time_effects=False)\n",
        "#     if panel_control == \"mundlak\":\n",
        "#         return MundlakBackend(include_entity_means=True, include_intercept=True)\n",
        "#     # fallback pooled (or raise)\n",
        "#     return PooledOLSBackend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def make_model(cfg: RunConfig):\n",
        "#     if cfg.algorithm == \"OLS\":\n",
        "#         return LinearRegression()\n",
        "#     if cfg.algorithm == \"Ridge\":\n",
        "#         return Ridge(alpha=cfg.alpha, random_state=cfg.seed)\n",
        "#     if cfg.algorithm == \"ElasticNet\":\n",
        "#         return ElasticNet(alpha=cfg.alpha, l1_ratio=cfg.l1_ratio, random_state=cfg.seed, max_iter=10000)\n",
        "#     raise ValueError(f\"Unknown algorithm: {cfg.algorithm}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # =========================\n",
        "# # 6) Minimal usage demo (fit only; MLflow logging stays in your run_one_experiment)\n",
        "# # =========================\n",
        "# def fit_one_trial_no_mlflow(df_pd: pd.DataFrame, cfg: RunConfig) -> FitResult:\n",
        "#     \"\"\"\n",
        "#     Convenience function for testing the FE/Mundlak backends without MLflow.\n",
        "#     In your pipeline, you'd call make_fit_backend(...) inside run_one_experiment().\n",
        "#     \"\"\"\n",
        "#     train_df, _ = slice_by_time(df_pd, cfg.time_split)\n",
        "#     y_col = get_target_col(cfg.target_node)\n",
        "#     x_cols = list(cfg.features)\n",
        "\n",
        "#     backend = make_fit_backend(cfg.panel_control)\n",
        "#     return backend.fit(train_df, y_col=y_col, x_cols=x_cols, entity_col=\"store_id\", time_col=\"week_start\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def fit_and_get_coef_table(cfg: RunConfig, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "#     \"\"\"\n",
        "#     Returns: model_object, coef_df\n",
        "#     coef_df columns: feature, coef, sign, abs_coef, rank_abscoef, t_stat, p_value\n",
        "#     \"\"\"\n",
        "#     feats = list(X_train.columns)\n",
        "\n",
        "#     if cfg.algorithm == \"OLS\":\n",
        "#         # statsmodels gives p-values\n",
        "#         X_sm = sm.add_constant(X_train, has_constant=\"add\")\n",
        "#         model = sm.OLS(y_train, X_sm).fit()\n",
        "\n",
        "#         coef = model.params.drop(\"const\", errors=\"ignore\")\n",
        "#         tstat = model.tvalues.drop(\"const\", errors=\"ignore\")\n",
        "#         pval = model.pvalues.drop(\"const\", errors=\"ignore\")\n",
        "\n",
        "#         df = pd.DataFrame({\n",
        "#             \"feature\": coef.index,\n",
        "#             \"coef\": coef.values,\n",
        "#             \"t_stat\": tstat.reindex(coef.index).values,\n",
        "#             \"p_value\": pval.reindex(coef.index).values,\n",
        "#         })\n",
        "\n",
        "#     else:\n",
        "#         # sklearn (no p-values)\n",
        "#         model = make_model(cfg)\n",
        "#         model.fit(X_train, y_train)\n",
        "\n",
        "#         coefs = np.asarray(model.coef_).ravel()\n",
        "#         df = pd.DataFrame({\n",
        "#             \"feature\": feats,\n",
        "#             \"coef\": coefs,\n",
        "#             \"t_stat\": np.nan,\n",
        "#             \"p_value\": np.nan,\n",
        "#         })\n",
        "\n",
        "#     df[\"abs_coef\"] = df[\"coef\"].abs()\n",
        "#     df[\"sign\"] = np.sign(df[\"coef\"]).astype(int)\n",
        "#     df = df.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
        "#     df[\"rank_abscoef\"] = np.arange(1, len(df) + 1)\n",
        "\n",
        "#     # simple significance flag (customize threshold later)\n",
        "#     df[\"is_significant_05\"] = (df[\"p_value\"] < 0.05)\n",
        "\n",
        "#     return model, df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5) Data prep component (you will replace pieces later)\n",
        "\n",
        "Assumes:\n",
        "\n",
        "df_pd includes store_id, week_start, plus feature columns\n",
        "\n",
        "target columns exist, e.g. log_GC, log_AC (or you can compute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def slice_by_time(df: pd.DataFrame, ts: TimeSplit) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    df = df.copy()\n",
        "    df[\"week_start\"] = pd.to_datetime(df[\"week_start\"])\n",
        "    train = df[(df[\"week_start\"] >= ts.train_start) & (df[\"week_start\"] <= ts.train_end)]\n",
        "    test  = df[(df[\"week_start\"] >= ts.test_start)  & (df[\"week_start\"] <= ts.test_end)]\n",
        "    return train, test\n",
        "\n",
        "def get_target_col(target_node: str) -> str:\n",
        "    # you can change this mapping anytime\n",
        "    if target_node == \"GC\":\n",
        "        return \"log_GC\"\n",
        "    if target_node == \"AC\":\n",
        "        return \"log_AC\"\n",
        "    raise ValueError(\"target_node must be 'GC' or 'AC'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6) Logging policy component (what to log)\n",
        "\n",
        "Keep parameters as params; use tags for “indexing / grouping” fields you’ll filter on later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_run_inputs(cfg: RunConfig) -> None:\n",
        "    # Params (queryable, shown in UI)\n",
        "    mlflow.log_params({\n",
        "        \"target_node\": cfg.target_node,\n",
        "        \"panel_control\": cfg.panel_control,\n",
        "        \"algorithm\": cfg.algorithm,\n",
        "        \"alpha\": cfg.alpha,\n",
        "        \"l1_ratio\": cfg.l1_ratio,\n",
        "        \"feature_block_set_id\": cfg.feature_block_set_id,\n",
        "        \"n_features\": len(cfg.features),\n",
        "        \"time_split_id\": cfg.time_split.time_split_id,\n",
        "        \"train_start\": cfg.time_split.train_start,\n",
        "        \"train_end\": cfg.time_split.train_end,\n",
        "        \"test_start\": cfg.time_split.test_start,\n",
        "        \"test_end\": cfg.time_split.test_end,\n",
        "        \"seed\": cfg.seed,\n",
        "    })\n",
        "\n",
        "def log_metrics(prefix: str, y_true, y_pred) -> None:\n",
        "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mlflow.log_metrics({\n",
        "        f\"{prefix}_rmse\": rmse,\n",
        "        f\"{prefix}_mae\": mae,\n",
        "        f\"{prefix}_r2\": r2,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7) Fit + log artifacts (coeff table, rank table, config snapshot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def coef_table(model, feature_names: List[str]) -> pd.DataFrame:\n",
        "    # Works for linear models that have coef_\n",
        "    coefs = np.asarray(model.coef_).ravel()\n",
        "    df = pd.DataFrame({\"feature\": feature_names, \"coef\": coefs})\n",
        "    df[\"abs_coef\"] = df[\"coef\"].abs()\n",
        "    df[\"sign\"] = np.sign(df[\"coef\"]).astype(int)\n",
        "    df = df.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n",
        "    df[\"rank_abscoef\"] = np.arange(1, len(df) + 1)\n",
        "    return df\n",
        "\n",
        "def log_dataframe_as_csv(df: pd.DataFrame, artifact_path: str, filename: str) -> None:\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        fpath = os.path.join(tmpdir, filename)\n",
        "        df.to_csv(fpath, index=False)\n",
        "        mlflow.log_artifact(fpath, artifact_path=artifact_path)\n",
        "\n",
        "def log_config_snapshot(cfg: RunConfig) -> None:\n",
        "    payload = {\n",
        "        \"target_node\": cfg.target_node,\n",
        "        \"panel_control\": cfg.panel_control,\n",
        "        \"algorithm\": cfg.algorithm,\n",
        "        \"alpha\": cfg.alpha,\n",
        "        \"l1_ratio\": cfg.l1_ratio,\n",
        "        \"feature_block_set_id\": cfg.feature_block_set_id,\n",
        "        \"features\": list(cfg.features),\n",
        "        \"time_split\": cfg.time_split.__dict__,\n",
        "        \"seed\": cfg.seed,\n",
        "    }\n",
        "    mlflow.log_dict(payload, artifact_file=\"run_config.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8) Single run execution (with failure handling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def run_one_experiment(df_pd: pd.DataFrame, cfg: RunConfig) -> None:\n",
        "#     run_name = f\"{cfg.target_node}__{cfg.panel_control}__{cfg.algorithm}__{cfg.feature_block_set_id}__{cfg.time_split.time_split_id}\"\n",
        "\n",
        "#     with mlflow.start_run(run_name=run_name, nested=True):\n",
        "#         try:\n",
        "#             log_run_inputs(cfg)\n",
        "#             log_config_snapshot(cfg)\n",
        "\n",
        "#             # Data slicing\n",
        "#             train_df, test_df = slice_by_time(df_pd, cfg.time_split)\n",
        "#             ycol = get_target_col(cfg.target_node)\n",
        "\n",
        "#             X_train = train_df.loc[:, list(cfg.features)]\n",
        "#             y_train = train_df[ycol]\n",
        "#             X_test  = test_df.loc[:, list(cfg.features)]\n",
        "#             y_test  = test_df[ycol]\n",
        "\n",
        "#             # Fit\n",
        "#             model = make_model(cfg)\n",
        "#             model.fit(X_train, y_train)\n",
        "\n",
        "#             # Predict + metrics\n",
        "#             pred_train = model.predict(X_train)\n",
        "#             pred_test  = model.predict(X_test)\n",
        "#             log_metrics(\"train\", y_train, pred_train)\n",
        "#             log_metrics(\"test\", y_test, pred_test)\n",
        "\n",
        "#             # Artifacts: coefficients + ranks\n",
        "#             cdf = coef_table(model, list(cfg.features))\n",
        "#             log_dataframe_as_csv(cdf, artifact_path=\"artifacts\", filename=\"coefficients.csv\")\n",
        "\n",
        "#             # Optional: log model (safe to keep optional if volume is huge)\n",
        "#             # mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             mlflow.set_tag(\"run_status\", \"failed\")\n",
        "#             mlflow.log_text(str(e), \"error.txt\")\n",
        "#             raise\n",
        "#         else:\n",
        "#             mlflow.set_tag(\"run_status\", \"ok\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Active run before: <ActiveRun: >\n",
            "Active run entering child: <ActiveRun: >\n"
          ]
        }
      ],
      "source": [
        "# print(\"Active run before:\", mlflow.active_run())\n",
        "# print(\"Active run entering child:\", mlflow.active_run())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def run_one_experiment(df_pd: pd.DataFrame, cfg: RunConfig) -> None:\n",
        "#     \"\"\"\n",
        "#     Execute ONE fully-specified trial and log everything needed for later stability aggregation.\n",
        "\n",
        "#     Logs (per run):\n",
        "#       - params: target_node, panel_control, algorithm, hyperparams, feature_set_id, n_features, time split, etc.\n",
        "#       - metrics: train/test r2/rmse/mae\n",
        "#       - artifacts: coefficients.csv (coef/sign/rank + p-values for OLS), run_config.json\n",
        "#       - tags: run_status\n",
        "#     \"\"\"\n",
        "#     run_name = (\n",
        "#         f\"{cfg.target_node}__{cfg.panel_control}__{cfg.algorithm}\"\n",
        "#         f\"__{cfg.feature_block_set_id}__{cfg.time_split.time_split_id}\"\n",
        "#     )\n",
        "\n",
        "#     with mlflow.start_run(run_name=run_name, nested=True):\n",
        "#         mlflow.set_tag(\"run_type\", \"trial\")\n",
        "#         try:\n",
        "#             # --- 1) Log inputs (params + config snapshot) ---\n",
        "#             log_run_inputs(cfg)\n",
        "#             log_config_snapshot(cfg)\n",
        "\n",
        "#             # --- 2) Slice data ---\n",
        "#             train_df, test_df = slice_by_time(df_pd, cfg.time_split)\n",
        "#             ycol = get_target_col(cfg.target_node)\n",
        "\n",
        "#             # Basic guards (kept simple)\n",
        "#             if len(cfg.features) == 0:\n",
        "#                 raise ValueError(\"cfg.features is empty.\")\n",
        "#             missing_cols = [c for c in cfg.features + (ycol,) if c not in train_df.columns]\n",
        "#             if missing_cols:\n",
        "#                 raise ValueError(f\"Missing columns in df_pd: {missing_cols}\")\n",
        "\n",
        "#             X_train = train_df.loc[:, list(cfg.features)]\n",
        "#             y_train = train_df[ycol]\n",
        "#             X_test = test_df.loc[:, list(cfg.features)]\n",
        "#             y_test = test_df[ycol]\n",
        "\n",
        "#             # --- 3) Fit + build coefficients table (with p-values only for OLS) ---\n",
        "#             model, cdf = fit_and_get_coef_table(cfg, X_train, y_train)\n",
        "\n",
        "#             # Add run context directly into the coefficient table (helps aggregation later)\n",
        "#             cdf[\"target_node\"] = cfg.target_node\n",
        "#             cdf[\"panel_control\"] = cfg.panel_control\n",
        "#             cdf[\"algorithm\"] = cfg.algorithm\n",
        "#             cdf[\"feature_block_set_id\"] = cfg.feature_block_set_id\n",
        "#             cdf[\"time_split_id\"] = cfg.time_split.time_split_id\n",
        "\n",
        "#             # --- 4) Predict (handle statsmodels vs sklearn) ---\n",
        "#             if cfg.algorithm == \"OLS\":\n",
        "#                 pred_train = model.predict(sm.add_constant(X_train, has_constant=\"add\"))\n",
        "#                 pred_test = model.predict(sm.add_constant(X_test, has_constant=\"add\"))\n",
        "#             else:\n",
        "#                 pred_train = model.predict(X_train)\n",
        "#                 pred_test = model.predict(X_test)\n",
        "\n",
        "#             # --- 5) Log metrics ---\n",
        "#             log_metrics(\"train\", y_train, pred_train)\n",
        "#             log_metrics(\"test\", y_test, pred_test)\n",
        "\n",
        "#             # Optional: log a couple simple summary metrics from coefficient table\n",
        "#             # (e.g. number significant; works for OLS only)\n",
        "#             if cdf[\"p_value\"].notna().any():\n",
        "#                 mlflow.log_metric(\"n_significant_05\", float((cdf[\"p_value\"] < 0.05).sum()))\n",
        "#                 mlflow.log_metric(\"pct_significant_05\", float((cdf[\"p_value\"] < 0.05).mean()))\n",
        "#             else:\n",
        "#                 mlflow.log_metric(\"n_significant_05\", np.nan)\n",
        "#                 mlflow.log_metric(\"pct_significant_05\", np.nan)\n",
        "\n",
        "#             # For ElasticNet, you may care about selection rate (non-zero)\n",
        "#             if cfg.algorithm == \"ElasticNet\":\n",
        "#                 mlflow.log_metric(\"n_nonzero_coef\", float((cdf[\"coef\"].abs() > 1e-12).sum()))\n",
        "#                 mlflow.log_metric(\"pct_nonzero_coef\", float((cdf[\"coef\"].abs() > 1e-12).mean()))\n",
        "\n",
        "#             # --- 6) Log artifacts (system of record for per-feature consolidation) ---\n",
        "#             log_dataframe_as_csv(cdf, artifact_path=\"artifacts\", filename=\"coefficients.csv\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             mlflow.set_tag(\"run_status\", \"failed\")\n",
        "#             mlflow.log_text(str(e), \"error.txt\")\n",
        "#             raise\n",
        "#         else:\n",
        "#             mlflow.set_tag(\"run_status\", \"ok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import numpy as np\n",
        "\n",
        "def run_one_experiment(df_pd: pd.DataFrame, cfg: RunConfig) -> None:\n",
        "    run_name = (\n",
        "        f\"{cfg.target_node}__{cfg.panel_control}__{cfg.algorithm}\"\n",
        "        f\"__{cfg.feature_block_set_id}__{cfg.time_split.time_split_id}\"\n",
        "    )\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name, nested=True):\n",
        "        mlflow.set_tag(\"run_type\", \"trial\")\n",
        "        try:\n",
        "            # 1) log inputs\n",
        "            log_run_inputs(cfg)\n",
        "            log_config_snapshot(cfg)\n",
        "\n",
        "            # 2) slice data\n",
        "            train_df, test_df = slice_by_time(df_pd, cfg.time_split)\n",
        "            ycol = get_target_col(cfg.target_node)\n",
        "\n",
        "            if len(cfg.features) == 0:\n",
        "                raise ValueError(\"cfg.features is empty.\")\n",
        "\n",
        "            needed = list(cfg.features) + [ycol, \"store_id\", \"week_start\"]\n",
        "            missing = [c for c in needed if c not in train_df.columns]\n",
        "            if missing:\n",
        "                raise ValueError(f\"Missing columns: {missing}\")\n",
        "\n",
        "            x_cols = list(cfg.features)\n",
        "\n",
        "            # 3) fit using backend selector (FE/Mundlak/pooled)\n",
        "            fit_res = fit_with_backend(train_df, ycol, x_cols, cfg)\n",
        "\n",
        "            # 4) predictions + metrics\n",
        "            pred_train = fit_res.predict_fn(train_df)\n",
        "            pred_test  = fit_res.predict_fn(test_df)\n",
        "\n",
        "            y_train = train_df[ycol].astype(float).to_numpy()\n",
        "            y_test  = test_df[ycol].astype(float).to_numpy()\n",
        "\n",
        "            log_metrics(\"train\", y_train, pred_train)\n",
        "            log_metrics(\"test\", y_test, pred_test)\n",
        "\n",
        "            # 5) add run context to coef table + log artifact\n",
        "            cdf = fit_res.coef_df.copy()\n",
        "            cdf[\"target_node\"] = cfg.target_node\n",
        "            cdf[\"panel_control\"] = cfg.panel_control\n",
        "            cdf[\"algorithm\"] = cfg.algorithm\n",
        "            cdf[\"feature_block_set_id\"] = cfg.feature_block_set_id\n",
        "            cdf[\"time_split_id\"] = cfg.time_split.time_split_id\n",
        "\n",
        "            # optional summary metrics\n",
        "            if cdf[\"p_value\"].notna().any():\n",
        "                mlflow.log_metric(\"n_significant_05\", float((cdf[\"p_value\"] < 0.05).sum()))\n",
        "                mlflow.log_metric(\"pct_significant_05\", float((cdf[\"p_value\"] < 0.05).mean()))\n",
        "            else:\n",
        "                mlflow.log_metric(\"n_significant_05\", np.nan)\n",
        "                mlflow.log_metric(\"pct_significant_05\", np.nan)\n",
        "\n",
        "            if cfg.algorithm == \"ElasticNet\":\n",
        "                mlflow.log_metric(\"n_nonzero_coef\", float((cdf[\"coef\"].abs() > 1e-12).sum()))\n",
        "                mlflow.log_metric(\"pct_nonzero_coef\", float((cdf[\"coef\"].abs() > 1e-12).mean()))\n",
        "\n",
        "            log_dataframe_as_csv(cdf, artifact_path=\"artifacts\", filename=\"coefficients.csv\")\n",
        "            # NEW: log summary for all models\n",
        "            summary_text = get_model_summary_text(fit_res.model)\n",
        "            mlflow.log_text(summary_text, \"artifacts/model_summary.txt\")\n",
        "\n",
        "        except Exception as e:\n",
        "            mlflow.set_tag(\"run_status\", \"failed\")\n",
        "            mlflow.log_text(str(e), \"error.txt\")\n",
        "            raise\n",
        "        else:\n",
        "            mlflow.set_tag(\"run_status\", \"ok\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Orchestrator (parent run + many child runs)\n",
        "\n",
        "This matches the MLflow tutorial pattern (parent run contains the “study”, child runs contain each trial)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def run_study(\n",
        "#     df_pd: pd.DataFrame,\n",
        "#     experiment_name: str,\n",
        "#     study_name: str,\n",
        "#     common_tags: Dict[str, str],\n",
        "#     configs: Iterator[RunConfig],\n",
        "#     max_runs: int = None,\n",
        "# ) -> str:\n",
        "#     setup_mlflow(experiment_name, common_tags)\n",
        "\n",
        "#     with mlflow.start_run(run_name=study_name) as parent:\n",
        "#         mlflow.set_tag(\"run_type\", \"study\")\n",
        "#         mlflow.log_param(\"study_name\", study_name)\n",
        "\n",
        "#         n = 0\n",
        "#         for cfg in configs:\n",
        "#             if max_runs is not None and n >= max_runs:\n",
        "#                 break\n",
        "#             run_one_experiment(df_pd, cfg)\n",
        "#             n += 1\n",
        "\n",
        "#         mlflow.log_param(\"n_child_runs\", n)\n",
        "#         return parent.info.run_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def run_study(\n",
        "#     df_pd: pd.DataFrame,\n",
        "#     experiment_name: str,\n",
        "#     study_name: str,\n",
        "#     common_tags: Dict[str, str],\n",
        "#     configs: Iterator[RunConfig],\n",
        "#     max_runs: int = None,\n",
        "# ) -> str:\n",
        "#     # extra safety for notebooks: close anything dangling\n",
        "#     while mlflow.active_run() is not None:\n",
        "#         mlflow.end_run()\n",
        "\n",
        "#     setup_mlflow(experiment_name)\n",
        "\n",
        "#     # start parent run FIRST, then set tags\n",
        "#     with mlflow.start_run(run_name=study_name) as parent:\n",
        "#         mlflow.set_tags(common_tags)          # ✅ now safe\n",
        "#         mlflow.set_tag(\"run_type\", \"study\")\n",
        "#         mlflow.log_param(\"study_name\", study_name)\n",
        "\n",
        "#         n = 0\n",
        "#         for cfg in configs:\n",
        "#             if max_runs is not None and n >= max_runs:\n",
        "#                 break\n",
        "#             run_one_experiment(df_pd, cfg)     # child run MUST be nested=True\n",
        "#             n += 1\n",
        "\n",
        "#         mlflow.log_param(\"n_child_runs\", n)\n",
        "#         return parent.info.run_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, hashlib\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "from typing import Dict, Iterator, Optional\n",
        "\n",
        "def _hash_dict(d: Dict) -> str:\n",
        "    s = json.dumps(d, sort_keys=True)\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:12]\n",
        "\n",
        "def run_study(\n",
        "    df_pd: pd.DataFrame,\n",
        "    experiment_name: str,\n",
        "    study_name: str,\n",
        "    common_tags: Dict[str, str],\n",
        "    configs: Iterator[RunConfig],\n",
        "    fe_meta: Optional[Dict] = None,\n",
        "    max_runs: int = None,\n",
        ") -> str:\n",
        "    while mlflow.active_run() is not None:\n",
        "        mlflow.end_run()\n",
        "\n",
        "    setup_mlflow(experiment_name)\n",
        "\n",
        "    with mlflow.start_run(run_name=study_name) as parent:\n",
        "        mlflow.set_tags(common_tags)\n",
        "        mlflow.set_tag(\"run_type\", \"study\")\n",
        "        mlflow.log_param(\"study_name\", study_name)\n",
        "\n",
        "        fe_meta_id = None\n",
        "        if fe_meta is not None:\n",
        "            fe_meta_id = _hash_dict(fe_meta)\n",
        "            mlflow.log_param(\"fe_meta_id\", fe_meta_id)\n",
        "            mlflow.log_dict(fe_meta, \"feature_engineering/log_transforms.json\")\n",
        "\n",
        "        n = 0\n",
        "        for cfg in configs:\n",
        "            if max_runs is not None and n >= max_runs:\n",
        "                break\n",
        "            # # pass the id into the child if you want (or store it globally)\n",
        "            # run_one_experiment(df_pd, cfg, fe_meta_id=fe_meta_id)\n",
        "            n += 1\n",
        "\n",
        "        mlflow.log_param(\"n_child_runs\", n)\n",
        "        return parent.info.run_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10) Retrieve results + feature stability summary (simple version)\n",
        "\n",
        "This is the “later I’ll decide which features to keep” part — implemented minimally using MLflow search + the logged coefficient artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_coefficients_for_run(run_id: str) -> pd.DataFrame:\n",
        "    # Download the artifact and read it\n",
        "    local_dir = mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"artifacts/coefficients.csv\")\n",
        "    return pd.read_csv(local_dir)\n",
        "\n",
        "def aggregate_feature_stability(\n",
        "    experiment_name: str,\n",
        "    filter_query: str = \"tags.run_status = 'ok'\",\n",
        ") -> pd.DataFrame:\n",
        "    exp = mlflow.get_experiment_by_name(experiment_name)\n",
        "    runs = mlflow.search_runs(experiment_ids=[exp.experiment_id], filter_string=filter_query)\n",
        "\n",
        "    rows = []\n",
        "    for _, r in runs.iterrows():\n",
        "        run_id = r[\"run_id\"]\n",
        "        try:\n",
        "            cdf = load_coefficients_for_run(run_id)\n",
        "            cdf[\"run_id\"] = run_id\n",
        "            rows.append(cdf[[\"run_id\", \"feature\", \"coef\", \"sign\", \"abs_coef\", \"rank_abscoef\"]])\n",
        "        except Exception:\n",
        "            # If some run didn't log artifacts, skip (or handle stricter)\n",
        "            continue\n",
        "\n",
        "    if not rows:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    allc = pd.concat(rows, ignore_index=True)\n",
        "\n",
        "    # Stability stats you described (simple baseline)\n",
        "    g = allc.groupby(\"feature\")\n",
        "    out = pd.DataFrame({\n",
        "        \"n_runs_appeared\": g[\"run_id\"].nunique(),\n",
        "        \"mean_coef\": g[\"coef\"].mean(),\n",
        "        \"median_coef\": g[\"coef\"].median(),\n",
        "        \"std_coef\": g[\"coef\"].std(ddof=1),\n",
        "        \"mean_abscoef\": g[\"abs_coef\"].mean(),\n",
        "        \"mean_rank\": g[\"rank_abscoef\"].mean(),\n",
        "        \"pct_positive\": g[\"sign\"].apply(lambda s: (s > 0).mean()),\n",
        "        \"pct_negative\": g[\"sign\"].apply(lambda s: (s < 0).mean()),\n",
        "    }).reset_index()\n",
        "\n",
        "    out[\"coef_cv\"] = out[\"std_coef\"] / out[\"mean_coef\"].replace(0, np.nan)\n",
        "    out = out.sort_values([\"n_runs_appeared\", \"mean_abscoef\"], ascending=[False, False]).reset_index(drop=True)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "11) Example usage (plug in your df_pd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>week_start</th>\n",
              "      <th>log_GC</th>\n",
              "      <th>digital_promo_1</th>\n",
              "      <th>digital_promo_2</th>\n",
              "      <th>digital_promo_3</th>\n",
              "      <th>digital_promo_4</th>\n",
              "      <th>digital_promo_5</th>\n",
              "      <th>media_1</th>\n",
              "      <th>media_2</th>\n",
              "      <th>media_3</th>\n",
              "      <th>media_4</th>\n",
              "      <th>media_5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-01-01</td>\n",
              "      <td>0.304717</td>\n",
              "      <td>-1.376686</td>\n",
              "      <td>0.232170</td>\n",
              "      <td>0.459386</td>\n",
              "      <td>1.403821</td>\n",
              "      <td>0.319400</td>\n",
              "      <td>0.044212</td>\n",
              "      <td>0.529413</td>\n",
              "      <td>-0.955625</td>\n",
              "      <td>0.417472</td>\n",
              "      <td>-1.180001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-01-08</td>\n",
              "      <td>-1.039984</td>\n",
              "      <td>0.635151</td>\n",
              "      <td>-0.555327</td>\n",
              "      <td>0.701954</td>\n",
              "      <td>-0.442536</td>\n",
              "      <td>-0.869047</td>\n",
              "      <td>-0.202914</td>\n",
              "      <td>1.363429</td>\n",
              "      <td>0.437512</td>\n",
              "      <td>-1.320489</td>\n",
              "      <td>0.804570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-01-15</td>\n",
              "      <td>0.750451</td>\n",
              "      <td>-0.222223</td>\n",
              "      <td>0.471539</td>\n",
              "      <td>0.138241</td>\n",
              "      <td>1.455046</td>\n",
              "      <td>0.177396</td>\n",
              "      <td>-1.082427</td>\n",
              "      <td>-1.880798</td>\n",
              "      <td>-1.241756</td>\n",
              "      <td>0.854686</td>\n",
              "      <td>-0.675114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-01-22</td>\n",
              "      <td>0.940565</td>\n",
              "      <td>-1.470806</td>\n",
              "      <td>1.012716</td>\n",
              "      <td>0.760133</td>\n",
              "      <td>0.131486</td>\n",
              "      <td>1.212519</td>\n",
              "      <td>-0.151052</td>\n",
              "      <td>-0.317907</td>\n",
              "      <td>-0.204069</td>\n",
              "      <td>-0.800212</td>\n",
              "      <td>0.403954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-01-29</td>\n",
              "      <td>-1.951035</td>\n",
              "      <td>-1.015579</td>\n",
              "      <td>0.155429</td>\n",
              "      <td>0.229211</td>\n",
              "      <td>0.258229</td>\n",
              "      <td>-0.323792</td>\n",
              "      <td>-0.746098</td>\n",
              "      <td>-0.867005</td>\n",
              "      <td>0.109648</td>\n",
              "      <td>0.632858</td>\n",
              "      <td>0.565460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>2025-06-01</td>\n",
              "      <td>1.463303</td>\n",
              "      <td>-0.376156</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>1.628937</td>\n",
              "      <td>1.847825</td>\n",
              "      <td>-0.079730</td>\n",
              "      <td>0.555582</td>\n",
              "      <td>0.101926</td>\n",
              "      <td>-0.084851</td>\n",
              "      <td>-2.480709</td>\n",
              "      <td>-0.439988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>2025-06-08</td>\n",
              "      <td>-1.188763</td>\n",
              "      <td>-0.133823</td>\n",
              "      <td>-1.412766</td>\n",
              "      <td>-0.970150</td>\n",
              "      <td>-0.174173</td>\n",
              "      <td>1.797561</td>\n",
              "      <td>-0.622168</td>\n",
              "      <td>-0.762323</td>\n",
              "      <td>-1.600206</td>\n",
              "      <td>-0.996419</td>\n",
              "      <td>-2.955619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>2025-06-15</td>\n",
              "      <td>-0.639752</td>\n",
              "      <td>-1.374896</td>\n",
              "      <td>-2.310103</td>\n",
              "      <td>-0.887696</td>\n",
              "      <td>1.667888</td>\n",
              "      <td>0.894213</td>\n",
              "      <td>0.987405</td>\n",
              "      <td>-0.859206</td>\n",
              "      <td>-0.761974</td>\n",
              "      <td>1.232902</td>\n",
              "      <td>-1.247317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>2025-06-22</td>\n",
              "      <td>-0.926576</td>\n",
              "      <td>-0.238174</td>\n",
              "      <td>0.054354</td>\n",
              "      <td>1.335784</td>\n",
              "      <td>-1.103741</td>\n",
              "      <td>0.011445</td>\n",
              "      <td>1.157508</td>\n",
              "      <td>-0.537663</td>\n",
              "      <td>0.148627</td>\n",
              "      <td>-2.777994</td>\n",
              "      <td>1.120841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>2025-06-29</td>\n",
              "      <td>-0.389810</td>\n",
              "      <td>-0.266387</td>\n",
              "      <td>-0.471776</td>\n",
              "      <td>-0.191344</td>\n",
              "      <td>0.587259</td>\n",
              "      <td>0.248787</td>\n",
              "      <td>1.436302</td>\n",
              "      <td>0.542594</td>\n",
              "      <td>0.366210</td>\n",
              "      <td>-0.347523</td>\n",
              "      <td>-0.664626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>131 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    week_start    log_GC  digital_promo_1  digital_promo_2  digital_promo_3  \\\n",
              "0   2023-01-01  0.304717        -1.376686         0.232170         0.459386   \n",
              "1   2023-01-08 -1.039984         0.635151        -0.555327         0.701954   \n",
              "2   2023-01-15  0.750451        -0.222223         0.471539         0.138241   \n",
              "3   2023-01-22  0.940565        -1.470806         1.012716         0.760133   \n",
              "4   2023-01-29 -1.951035        -1.015579         0.155429         0.229211   \n",
              "..         ...       ...              ...              ...              ...   \n",
              "126 2025-06-01  1.463303        -0.376156         0.276274         1.628937   \n",
              "127 2025-06-08 -1.188763        -0.133823        -1.412766        -0.970150   \n",
              "128 2025-06-15 -0.639752        -1.374896        -2.310103        -0.887696   \n",
              "129 2025-06-22 -0.926576        -0.238174         0.054354         1.335784   \n",
              "130 2025-06-29 -0.389810        -0.266387        -0.471776        -0.191344   \n",
              "\n",
              "     digital_promo_4  digital_promo_5   media_1   media_2   media_3   media_4  \\\n",
              "0           1.403821         0.319400  0.044212  0.529413 -0.955625  0.417472   \n",
              "1          -0.442536        -0.869047 -0.202914  1.363429  0.437512 -1.320489   \n",
              "2           1.455046         0.177396 -1.082427 -1.880798 -1.241756  0.854686   \n",
              "3           0.131486         1.212519 -0.151052 -0.317907 -0.204069 -0.800212   \n",
              "4           0.258229        -0.323792 -0.746098 -0.867005  0.109648  0.632858   \n",
              "..               ...              ...       ...       ...       ...       ...   \n",
              "126         1.847825        -0.079730  0.555582  0.101926 -0.084851 -2.480709   \n",
              "127        -0.174173         1.797561 -0.622168 -0.762323 -1.600206 -0.996419   \n",
              "128         1.667888         0.894213  0.987405 -0.859206 -0.761974  1.232902   \n",
              "129        -1.103741         0.011445  1.157508 -0.537663  0.148627 -2.777994   \n",
              "130         0.587259         0.248787  1.436302  0.542594  0.366210 -0.347523   \n",
              "\n",
              "      media_5  \n",
              "0   -1.180001  \n",
              "1    0.804570  \n",
              "2   -0.675114  \n",
              "3    0.403954  \n",
              "4    0.565460  \n",
              "..        ...  \n",
              "126 -0.439988  \n",
              "127 -2.955619  \n",
              "128 -1.247317  \n",
              "129  1.120841  \n",
              "130 -0.664626  \n",
              "\n",
              "[131 rows x 12 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import mlflow\n",
        "# while mlflow.active_run() is not None:\n",
        "#     mlflow.end_run()\n",
        "# print(\"Active run now:\", mlflow.active_run())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "local testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_blocks = {\n",
        "    \"promo\": ['digital_promo_1','digital_promo_2','digital_promo_3','digital_promo_4','digital_promo_5'],\n",
        "    \"media\": ['media_1','media_2','media_3','media_4','media_5'],\n",
        "}\n",
        "\n",
        "k_per_block = {\"promo\": 3, \"media\": 3}\n",
        "\n",
        "time_splits = [\n",
        "    TimeSplit(\"ts1\", \"2023-01-01\", \"2024-06-30\", \"2024-07-01\", \"2024-12-31\"),\n",
        "]\n",
        "\n",
        "algorithms = [\n",
        "    {\"name\": \"OLS\"},   # simplest to start (and gives p-values if you used statsmodels in OLS)\n",
        "]\n",
        "\n",
        "# feature_sets = generate_feature_sets(feature_blocks)\n",
        "\n",
        "feature_sets = sample_k_per_block(\n",
        "    feature_blocks=feature_blocks,\n",
        "    k_per_block=k_per_block,\n",
        "    n_samples=200,     # you control how many combinations\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# configs_gc = search_space(\n",
        "#     target_node=\"GC\",\n",
        "#     panel_controls=[\"FE\"],     # keep one for smoke test\n",
        "#     algorithms=algorithms,\n",
        "#     time_splits=time_splits,\n",
        "#     feature_sets={\"block__promo\": feature_sets[\"block__promo\"]},  # one feature set only\n",
        "#     seed=42\n",
        "# )\n",
        "\n",
        "configs_gc = search_space_with_feature_sets(\n",
        "    target_node=\"GC\",\n",
        "    panel_controls=[\"FE\", \"Mundlak\"],\n",
        "    algorithms=[{\"name\":\"OLS\"}, {\"name\":\"Ridge\",\"alpha\":1.0}, {\"name\":\"ElasticNet\",\"alpha\":0.1,\"l1_ratio\":0.5}],\n",
        "    time_splits=time_splits,\n",
        "    feature_sets=feature_sets,\n",
        "    seed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Active run before: None\n",
            "           feature  n_runs_appeared  mean_coef  median_coef  std_coef  \\\n",
            "0  digital_promo_1                2  -0.184221    -0.184221       0.0   \n",
            "1  digital_promo_5                2  -0.107128    -0.107128       0.0   \n",
            "2  digital_promo_4                2  -0.052290    -0.052290       0.0   \n",
            "3  digital_promo_3                2  -0.023930    -0.023930       0.0   \n",
            "4  digital_promo_2                2   0.011568     0.011568       0.0   \n",
            "\n",
            "   mean_abscoef  mean_rank  pct_positive  pct_negative  coef_cv  \n",
            "0      0.184221        1.0           0.0           1.0     -0.0  \n",
            "1      0.107128        2.0           0.0           1.0     -0.0  \n",
            "2      0.052290        3.0           0.0           1.0     -0.0  \n",
            "3      0.023930        4.0           0.0           1.0     -0.0  \n",
            "4      0.011568        5.0           1.0           0.0      0.0  \n"
          ]
        }
      ],
      "source": [
        "print(\"Active run before:\", mlflow.active_run())\n",
        "\n",
        "parent_run_id = run_study(\n",
        "    df_pd=df,\n",
        "    experiment_name=\"RGA_Regression_Local\",\n",
        "    study_name=\"GC_smoketest_v001\",\n",
        "    common_tags={\n",
        "        \"project\": \"RevenueGrowthAnalytics\",\n",
        "        \"layer\": \"2\",\n",
        "        \"framework\": \"regression_shell\",\n",
        "        \"env\": \"local\",\n",
        "    },\n",
        "    configs=configs_gc,\n",
        "    # fe_meta = None,\n",
        "    max_runs=2,   # <-- smallest smoke test\n",
        ")\n",
        "\n",
        "stability = aggregate_feature_stability(\n",
        "    experiment_name=\"RGA_Regression_Local\",\n",
        "    filter_query=\"tags.run_status = 'ok' and params.target_node = 'GC'\"\n",
        ")\n",
        "\n",
        "print(stability.head(30))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# aggregate_feature_stability() slice_by_time(), fit_and_get_coef_table(), and generate_feature_sets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "complex version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example component inputs (you will replace these)\n",
        "# feature_blocks = {\n",
        "#     \"promo\": [\"promo_depth\", \"lto_flag\", \"discount_idx\", \"bundle_idx\", \"coupon_rate\"],\n",
        "#     \"media\": [\"tv_grps\", \"digital_imps\", \"search_spend\", \"social_spend\", \"ooh_spend\"],\n",
        "# }\n",
        "\n",
        "feature_blocks = {\n",
        "    \"promo\": ['digital_promo_1', 'digital_promo_2', 'digital_promo_3',\n",
        "       'digital_promo_4', 'digital_promo_5'],\n",
        "    \"media\": ['media_1', 'media_2', 'media_3', 'media_4',\n",
        "       'media_5'],\n",
        "}\n",
        "\n",
        "time_splits = [\n",
        "    TimeSplit(\"ts1\", \"2023-01-01\", \"2024-06-30\", \"2024-07-01\", \"2024-12-31\"),\n",
        "    TimeSplit(\"ts2\", \"2023-07-01\", \"2024-12-31\", \"2025-01-01\", \"2025-06-30\"),\n",
        "]\n",
        "\n",
        "algorithms = [\n",
        "    {\"name\": \"OLS\"},\n",
        "    {\"name\": \"Ridge\", \"alpha\": 1.0},\n",
        "    {\"name\": \"ElasticNet\", \"alpha\": 0.1, \"l1_ratio\": 0.5},\n",
        "]\n",
        "\n",
        "feature_sets = generate_feature_sets(feature_blocks)\n",
        "\n",
        "configs_gc = search_space(\n",
        "    target_node=\"GC\",\n",
        "    panel_controls=[\"FE\", \"Mundlak\"],      # Bayesian later\n",
        "    algorithms=algorithms,\n",
        "    time_splits=time_splits,\n",
        "    feature_sets=feature_sets,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "parent_run_id = run_study(\n",
        "    df_pd=df,\n",
        "    # experiment_name=\"/Users/your.name@company.com/RGA_Regression\",\n",
        "    experiment_name=\"RGA_Regression_Local\",\n",
        "    study_name=\"GC_study_v001\",\n",
        "    common_tags={\n",
        "        \"project\": \"RevenueGrowthAnalytics\",\n",
        "        \"layer\": \"2\",\n",
        "        \"framework\": \"regression_shell\",\n",
        "    },\n",
        "    configs=configs_gc,\n",
        "    max_runs=50,   # remove later\n",
        ")\n",
        "\n",
        "# Aggregate stability\n",
        "stability = aggregate_feature_stability(\n",
        "    experiment_name=\"/Users/your.name@company.com/RGA_Regression\",\n",
        "    filter_query=\"tags.run_status = 'ok' and params.target_node = 'GC'\"\n",
        ")\n",
        "\n",
        "display(stability.head(30))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick fast testing run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any, List, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "def evaluate_predictions(y_true, y_pred) -> Dict[str, float]:\n",
        "    y_true = np.asarray(y_true, dtype=float)\n",
        "    y_pred = np.asarray(y_pred, dtype=float)\n",
        "    return {\n",
        "        \"r2\": float(r2_score(y_true, y_pred)),\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "    }\n",
        "\n",
        "def run_trial_quick(\n",
        "    df_pd: pd.DataFrame,\n",
        "    cfg: RunConfig,\n",
        "    features: List[str],\n",
        "    time_split: Optional[TimeSplit] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    No MLflow. Uses the same backends and returns outputs for interpretation.\n",
        "    \"\"\"\n",
        "    ts = time_split or cfg.time_split\n",
        "    train_df, test_df = slice_by_time(df_pd, ts)\n",
        "    ycol = get_target_col(cfg.target_node)\n",
        "\n",
        "    fit_res = fit_with_backend(train_df, ycol, features, cfg)\n",
        "\n",
        "    y_train = train_df[ycol].astype(float).to_numpy()\n",
        "    y_test  = test_df[ycol].astype(float).to_numpy()\n",
        "    pred_train = fit_res.predict_fn(train_df)\n",
        "    pred_test  = fit_res.predict_fn(test_df)\n",
        "\n",
        "    out = {\n",
        "        \"run_name\": f\"{cfg.target_node}__{cfg.panel_control}__{cfg.algorithm}\",\n",
        "        \"cfg\": cfg,\n",
        "        \"features\": features,\n",
        "        \"metrics_train\": evaluate_predictions(y_train, pred_train),\n",
        "        \"metrics_test\": evaluate_predictions(y_test, pred_test),\n",
        "        \"coef_df\": fit_res.coef_df.copy(),\n",
        "    }\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'r2': -0.0023760852386276454,\n",
              " 'rmse': 0.8521974064595924,\n",
              " 'mae': 0.6560787768008266}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>coef</th>\n",
              "      <th>t_stat</th>\n",
              "      <th>p_value</th>\n",
              "      <th>abs_coef</th>\n",
              "      <th>sign</th>\n",
              "      <th>rank_abscoef</th>\n",
              "      <th>is_significant_05</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>const</td>\n",
              "      <td>6.496353</td>\n",
              "      <td>1.440030</td>\n",
              "      <td>0.150260</td>\n",
              "      <td>6.496353</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>digital_promo_1__mean_by_entity</td>\n",
              "      <td>-1.370871</td>\n",
              "      <td>-0.673746</td>\n",
              "      <td>0.500672</td>\n",
              "      <td>1.370871</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>digital_promo_2__mean_by_entity</td>\n",
              "      <td>1.195040</td>\n",
              "      <td>1.975489</td>\n",
              "      <td>0.048564</td>\n",
              "      <td>1.195040</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>log_media_1__mean_by_entity</td>\n",
              "      <td>0.106466</td>\n",
              "      <td>0.680856</td>\n",
              "      <td>0.496165</td>\n",
              "      <td>0.106466</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>digital_promo_2</td>\n",
              "      <td>-0.081768</td>\n",
              "      <td>-0.640369</td>\n",
              "      <td>0.522121</td>\n",
              "      <td>0.081768</td>\n",
              "      <td>-1</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>log_media_2__mean_by_entity</td>\n",
              "      <td>0.078861</td>\n",
              "      <td>0.207154</td>\n",
              "      <td>0.835943</td>\n",
              "      <td>0.078861</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>log_media_2</td>\n",
              "      <td>-0.039498</td>\n",
              "      <td>-1.161537</td>\n",
              "      <td>0.245779</td>\n",
              "      <td>0.039498</td>\n",
              "      <td>-1</td>\n",
              "      <td>7</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>log_media_1</td>\n",
              "      <td>0.031741</td>\n",
              "      <td>0.716252</td>\n",
              "      <td>0.474050</td>\n",
              "      <td>0.031741</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>digital_promo_1</td>\n",
              "      <td>-0.011046</td>\n",
              "      <td>-0.108142</td>\n",
              "      <td>0.913911</td>\n",
              "      <td>0.011046</td>\n",
              "      <td>-1</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           feature      coef    t_stat   p_value  abs_coef  \\\n",
              "0                            const  6.496353  1.440030  0.150260  6.496353   \n",
              "1  digital_promo_1__mean_by_entity -1.370871 -0.673746  0.500672  1.370871   \n",
              "2  digital_promo_2__mean_by_entity  1.195040  1.975489  0.048564  1.195040   \n",
              "3      log_media_1__mean_by_entity  0.106466  0.680856  0.496165  0.106466   \n",
              "4                  digital_promo_2 -0.081768 -0.640369  0.522121  0.081768   \n",
              "5      log_media_2__mean_by_entity  0.078861  0.207154  0.835943  0.078861   \n",
              "6                      log_media_2 -0.039498 -1.161537  0.245779  0.039498   \n",
              "7                      log_media_1  0.031741  0.716252  0.474050  0.031741   \n",
              "8                  digital_promo_1 -0.011046 -0.108142  0.913911  0.011046   \n",
              "\n",
              "   sign  rank_abscoef  is_significant_05  \n",
              "0     1             1              False  \n",
              "1    -1             2              False  \n",
              "2     1             3               True  \n",
              "3     1             4              False  \n",
              "4    -1             5              False  \n",
              "5     1             6              False  \n",
              "6    -1             7              False  \n",
              "7     1             8              False  \n",
              "8    -1             9              False  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "result = run_trial_quick(\n",
        "    df_pd=df,\n",
        "    cfg=RunConfig(target_node=\"GC\", panel_control=\"Mundlak\", algorithm=\"OLS\", time_split=time_splits[0]),\n",
        "    features=[\"log_media_1\", \"log_media_2\", \"digital_promo_1\", \"digital_promo_2\"],\n",
        ")\n",
        "\n",
        "# result[\"metrics_test\"]\n",
        "display(result[\"metrics_test\"])\n",
        "display(result[\"coef_df\"].head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_many_quick(\n",
        "    df_pd: pd.DataFrame,\n",
        "    base_cfg: RunConfig,\n",
        "    feature_sets: Dict[str, List[str]],\n",
        ") -> pd.DataFrame:\n",
        "    rows = []\n",
        "    coef_tables = {}\n",
        "\n",
        "    for name, feats in feature_sets.items():\n",
        "        res = run_trial_quick(df_pd, base_cfg, feats)\n",
        "        rows.append({\n",
        "            \"spec_name\": name,\n",
        "            \"n_features\": len(feats),\n",
        "            **{f\"train_{k}\": v for k, v in res[\"metrics_train\"].items()},\n",
        "            **{f\"test_{k}\": v for k, v in res[\"metrics_test\"].items()},\n",
        "        })\n",
        "        coef_tables[name] = res[\"coef_df\"]\n",
        "\n",
        "    summary = pd.DataFrame(rows).sort_values(\"test_rmse\").reset_index(drop=True)\n",
        "    return summary, coef_tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spec_name</th>\n",
              "      <th>n_features</th>\n",
              "      <th>train_r2</th>\n",
              "      <th>train_rmse</th>\n",
              "      <th>train_mae</th>\n",
              "      <th>test_r2</th>\n",
              "      <th>test_rmse</th>\n",
              "      <th>test_mae</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>media_only</td>\n",
              "      <td>3</td>\n",
              "      <td>0.004644</td>\n",
              "      <td>0.871425</td>\n",
              "      <td>0.689732</td>\n",
              "      <td>-0.002294</td>\n",
              "      <td>0.852162</td>\n",
              "      <td>0.656405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>promo_plus_media</td>\n",
              "      <td>4</td>\n",
              "      <td>0.005929</td>\n",
              "      <td>0.870863</td>\n",
              "      <td>0.688108</td>\n",
              "      <td>-0.002376</td>\n",
              "      <td>0.852197</td>\n",
              "      <td>0.656079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>promo_only</td>\n",
              "      <td>3</td>\n",
              "      <td>0.002187</td>\n",
              "      <td>0.872500</td>\n",
              "      <td>0.690480</td>\n",
              "      <td>-0.004426</td>\n",
              "      <td>0.853068</td>\n",
              "      <td>0.656416</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          spec_name  n_features  train_r2  train_rmse  train_mae   test_r2  \\\n",
              "0        media_only           3  0.004644    0.871425   0.689732 -0.002294   \n",
              "1  promo_plus_media           4  0.005929    0.870863   0.688108 -0.002376   \n",
              "2        promo_only           3  0.002187    0.872500   0.690480 -0.004426   \n",
              "\n",
              "   test_rmse  test_mae  \n",
              "0   0.852162  0.656405  \n",
              "1   0.852197  0.656079  \n",
              "2   0.853068  0.656416  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>coef</th>\n",
              "      <th>t_stat</th>\n",
              "      <th>p_value</th>\n",
              "      <th>abs_coef</th>\n",
              "      <th>sign</th>\n",
              "      <th>rank_abscoef</th>\n",
              "      <th>is_significant_05</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>const</td>\n",
              "      <td>6.496353</td>\n",
              "      <td>1.440030</td>\n",
              "      <td>0.150260</td>\n",
              "      <td>6.496353</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>digital_promo_1__mean_by_entity</td>\n",
              "      <td>-1.370871</td>\n",
              "      <td>-0.673746</td>\n",
              "      <td>0.500672</td>\n",
              "      <td>1.370871</td>\n",
              "      <td>-1</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>digital_promo_2__mean_by_entity</td>\n",
              "      <td>1.195040</td>\n",
              "      <td>1.975489</td>\n",
              "      <td>0.048564</td>\n",
              "      <td>1.195040</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>log_media_1__mean_by_entity</td>\n",
              "      <td>0.106466</td>\n",
              "      <td>0.680856</td>\n",
              "      <td>0.496165</td>\n",
              "      <td>0.106466</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>digital_promo_2</td>\n",
              "      <td>-0.081768</td>\n",
              "      <td>-0.640369</td>\n",
              "      <td>0.522121</td>\n",
              "      <td>0.081768</td>\n",
              "      <td>-1</td>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>log_media_2__mean_by_entity</td>\n",
              "      <td>0.078861</td>\n",
              "      <td>0.207154</td>\n",
              "      <td>0.835943</td>\n",
              "      <td>0.078861</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>log_media_2</td>\n",
              "      <td>-0.039498</td>\n",
              "      <td>-1.161537</td>\n",
              "      <td>0.245779</td>\n",
              "      <td>0.039498</td>\n",
              "      <td>-1</td>\n",
              "      <td>7</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>log_media_1</td>\n",
              "      <td>0.031741</td>\n",
              "      <td>0.716252</td>\n",
              "      <td>0.474050</td>\n",
              "      <td>0.031741</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>digital_promo_1</td>\n",
              "      <td>-0.011046</td>\n",
              "      <td>-0.108142</td>\n",
              "      <td>0.913911</td>\n",
              "      <td>0.011046</td>\n",
              "      <td>-1</td>\n",
              "      <td>9</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           feature      coef    t_stat   p_value  abs_coef  \\\n",
              "0                            const  6.496353  1.440030  0.150260  6.496353   \n",
              "1  digital_promo_1__mean_by_entity -1.370871 -0.673746  0.500672  1.370871   \n",
              "2  digital_promo_2__mean_by_entity  1.195040  1.975489  0.048564  1.195040   \n",
              "3      log_media_1__mean_by_entity  0.106466  0.680856  0.496165  0.106466   \n",
              "4                  digital_promo_2 -0.081768 -0.640369  0.522121  0.081768   \n",
              "5      log_media_2__mean_by_entity  0.078861  0.207154  0.835943  0.078861   \n",
              "6                      log_media_2 -0.039498 -1.161537  0.245779  0.039498   \n",
              "7                      log_media_1  0.031741  0.716252  0.474050  0.031741   \n",
              "8                  digital_promo_1 -0.011046 -0.108142  0.913911  0.011046   \n",
              "\n",
              "   sign  rank_abscoef  is_significant_05  \n",
              "0     1             1              False  \n",
              "1    -1             2              False  \n",
              "2     1             3               True  \n",
              "3     1             4              False  \n",
              "4    -1             5              False  \n",
              "5     1             6              False  \n",
              "6    -1             7              False  \n",
              "7     1             8              False  \n",
              "8    -1             9              False  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_cfg = RunConfig(target_node=\"GC\", panel_control=\"Mundlak\", algorithm=\"OLS\", time_split=time_splits[0])\n",
        "\n",
        "feature_sets = {\n",
        "    \"promo_only\": [\"digital_promo_1\", \"digital_promo_2\", \"digital_promo_3\"],\n",
        "    \"media_only\": [\"log_media_1\", \"log_media_2\", \"log_media_3\"],\n",
        "    \"promo_plus_media\": [\"digital_promo_1\", \"digital_promo_2\", \"log_media_1\", \"log_media_2\"],\n",
        "}\n",
        "\n",
        "summary, coef_tables = run_many_quick(df, base_cfg, feature_sets)\n",
        "display(summary)\n",
        "display(coef_tables[\"promo_plus_media\"].head(15))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import mlflow\n",
        "# from mlflow.tracking import MlflowClient\n",
        "\n",
        "# print(\"Tracking URI:\", mlflow.get_tracking_uri())\n",
        "# print(\"Active run:\", mlflow.active_run())\n",
        "\n",
        "# client = MlflowClient()\n",
        "# exps = client.search_experiments()\n",
        "# print(\"Experiments found:\", [e.name for e in exps][:10])\n",
        "\n",
        "# print(TRACK_DIR)\n",
        "\n",
        "# import os, mlflow\n",
        "# TRACK_DIR = os.path.abspath(\"./mlruns_rga_test\")\n",
        "# mlflow.set_tracking_uri(\"file:///\" + TRACK_DIR.replace(\"\\\\\", \"/\"))\n",
        "# print(\"Tracking URI set to:\", mlflow.get_tracking_uri())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from mlflow.tracking import MlflowClient\n",
        "# client = MlflowClient()\n",
        "\n",
        "# exp = client.get_experiment_by_name(\"RGA_Regression_Local\")\n",
        "# print(\"Experiment:\", exp)\n",
        "\n",
        "# runs = client.search_runs([exp.experiment_id], max_results=5)\n",
        "# print(\"Found runs:\", len(runs))\n",
        "# print([r.data.tags.get(\"mlflow.runName\") for r in runs])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(mlflow.get_tracking_uri())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{}\n"
          ]
        }
      ],
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "client = MlflowClient()\n",
        "\n",
        "run = client.get_run(\"15a6649b2b244ec4b624b0d7bfc0afc2\")\n",
        "print(run.data.metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "X_train.shape[0] == 0\n",
        "X_test.shape[0] == 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "import mlflow\n",
        "import statsmodels.api as sm\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class FinalSpec:\n",
        "    name: str                  # e.g., \"GC_final_v001\"\n",
        "    target_node: str           # \"GC\" / \"AC\"\n",
        "    panel_control: str         # \"FE\" / \"Mundlak\" / \"Bayesian\"\n",
        "    algorithm: str             # \"OLS\" / \"Ridge\" / \"ElasticNet\"\n",
        "    alpha: float = 0.0\n",
        "    l1_ratio: float = 0.0\n",
        "    features: List[str] = None\n",
        "    train_start: str = None\n",
        "    train_end: str = None\n",
        "\n",
        "def train_final_and_log_model(df_pd, spec: FinalSpec, experiment_name: str, tags: Dict[str,str]):\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    run_name = spec.name\n",
        "    with mlflow.start_run(run_name=run_name):\n",
        "        # Tag this as a promoted/final run\n",
        "        mlflow.set_tags({**tags, \"run_stage\": \"final\", \"is_champion\": \"true\"})\n",
        "\n",
        "        # Params (full snapshot)\n",
        "        mlflow.log_params({\n",
        "            \"target_node\": spec.target_node,\n",
        "            \"panel_control\": spec.panel_control,\n",
        "            \"algorithm\": spec.algorithm,\n",
        "            \"alpha\": spec.alpha,\n",
        "            \"l1_ratio\": spec.l1_ratio,\n",
        "            \"n_features\": len(spec.features),\n",
        "            \"train_start\": spec.train_start,\n",
        "            \"train_end\": spec.train_end,\n",
        "        })\n",
        "\n",
        "        # Train slice\n",
        "        df = df_pd.copy()\n",
        "        df[\"week_start\"] = pd.to_datetime(df[\"week_start\"])\n",
        "        train_df = df[(df[\"week_start\"] >= spec.train_start) & (df[\"week_start\"] <= spec.train_end)]\n",
        "\n",
        "        ycol = \"log_GC\" if spec.target_node == \"GC\" else \"log_AC\"\n",
        "        X_train = train_df[spec.features]\n",
        "        y_train = train_df[ycol]\n",
        "\n",
        "        # Fit + coeff artifact (reuse your function)\n",
        "        cfg = RunConfig(\n",
        "            target_node=spec.target_node,\n",
        "            panel_control=spec.panel_control,\n",
        "            algorithm=spec.algorithm,\n",
        "            alpha=spec.alpha,\n",
        "            l1_ratio=spec.l1_ratio,\n",
        "            feature_block_set_id=\"FINAL\",\n",
        "            features=tuple(spec.features),\n",
        "            time_split=TimeSplit(\"FINAL\", spec.train_start, spec.train_end, spec.train_start, spec.train_end),\n",
        "            seed=42,\n",
        "        )\n",
        "\n",
        "        model, cdf = fit_and_get_coef_table(cfg, X_train, y_train)\n",
        "        log_dataframe_as_csv(cdf, artifact_path=\"artifacts\", filename=\"coefficients.csv\")\n",
        "\n",
        "        # Log model artifact (ONLY for final)\n",
        "        if spec.algorithm == \"OLS\":\n",
        "            # statsmodels model\n",
        "            mlflow.statsmodels.log_model(model, artifact_path=\"model\")\n",
        "        else:\n",
        "            # sklearn model\n",
        "            import mlflow.sklearn\n",
        "            mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
        "\n",
        "        # Optional: store the final feature list\n",
        "        mlflow.log_text(\"\\n\".join(spec.features), \"final_features.txt\")\n",
        "\n",
        "        return mlflow.active_run().info.run_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_gc = FinalSpec(\n",
        "    name=\"GC_final_v001\",\n",
        "    target_node=\"GC\",\n",
        "    panel_control=\"Mundlak\",\n",
        "    algorithm=\"Ridge\",\n",
        "    alpha=1.0,\n",
        "    features=[...],                 # your chosen stable features\n",
        "    train_start=\"2023-01-01\",\n",
        "    train_end=\"2025-06-30\",\n",
        ")\n",
        "\n",
        "run_id = train_final_and_log_model(\n",
        "    df_pd=df,\n",
        "    spec=final_gc,\n",
        "    experiment_name=\"RGA_Regression_Local\",\n",
        "    tags={\"project\":\"RevenueGrowthAnalytics\", \"layer\":\"2\"}\n",
        ")\n",
        "print(\"Final model run_id:\", run_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c1195de73a404f6a9e5befacc0862eb4 GC__FE__OLS__block__promo__ts1\n"
          ]
        }
      ],
      "source": [
        "runs = client.search_runs(\n",
        "    experiment_ids=[exp.experiment_id],\n",
        "    filter_string=\"tags.run_status = 'ok'\",\n",
        "    order_by=[\"attributes.start_time DESC\"],\n",
        "    max_results=5\n",
        ")\n",
        "\n",
        "for r in runs:\n",
        "    print(r.info.run_id, r.data.tags.get(\"mlflow.runName\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{}\n"
          ]
        }
      ],
      "source": [
        "run = client.get_run(\"41894288e1974559b68c3337fb586335\")\n",
        "print(run.data.metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8510b0a2bd9b431291963245548f5017 GC__FE__OLS__block__promo__ts1 0.7691974572947565\n"
          ]
        }
      ],
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "client = MlflowClient()\n",
        "\n",
        "exp = client.get_experiment_by_name(\"RGA_Regression_Local\")\n",
        "\n",
        "runs = client.search_runs(\n",
        "    experiment_ids=[exp.experiment_id],\n",
        "    filter_string=\"tags.run_status = 'ok' and tags.run_type != 'study'\",\n",
        "    order_by=[\"attributes.start_time DESC\"],\n",
        "    max_results=20,\n",
        ")\n",
        "\n",
        "for r in runs:\n",
        "    print(r.info.run_id, r.data.tags.get(\"mlflow.runName\"), r.data.metrics.get(\"test_rmse\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
