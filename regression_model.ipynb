{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# Panel definition\n",
    "# -----------------------------\n",
    "stores = [f\"S{i:02d}\" for i in range(1, 11)]\n",
    "weeks = pd.date_range(\"2023-01-02\", \"2025-12-29\", freq=\"W-MON\")\n",
    "\n",
    "panel = pd.MultiIndex.from_product(\n",
    "    [stores, weeks],\n",
    "    names=[\"store_id\", \"week_start\"]\n",
    ").to_frame(index=False)\n",
    "\n",
    "n = len(panel)\n",
    "\n",
    "# -----------------------------\n",
    "# Store-level latent effects\n",
    "# -----------------------------\n",
    "store_effect_gc = panel[\"store_id\"].map(\n",
    "    {s: np.random.normal(0, 0.15) for s in stores}\n",
    ")\n",
    "\n",
    "store_effect_ac = panel[\"store_id\"].map(\n",
    "    {s: np.random.normal(0, 0.10) for s in stores}\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: generate correlated features\n",
    "# -----------------------------\n",
    "def generate_domain(prefix, n_features, base_scale=1.0):\n",
    "    base = np.random.normal(0, base_scale, size=n)\n",
    "    data = {}\n",
    "    for i in range(1, n_features + 1):\n",
    "        data[f\"{prefix}_{i}\"] = base + np.random.normal(0, 0.5, size=n)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# -----------------------------\n",
    "# Feature domains (5 each)\n",
    "# -----------------------------\n",
    "digital_promo   = generate_domain(\"digital_promo\", 5)\n",
    "nondigital_promo = generate_domain(\"nondigital_promo\", 5)\n",
    "media            = generate_domain(\"media\", 5)\n",
    "menu             = generate_domain(\"menu\", 5)\n",
    "mix_share        = generate_domain(\"mix_share\", 5)\n",
    "csat             = generate_domain(\"csat\", 5)\n",
    "pricing          = generate_domain(\"pricing\", 5)\n",
    "\n",
    "X = pd.concat(\n",
    "    [\n",
    "        digital_promo,\n",
    "        nondigital_promo,\n",
    "        media,\n",
    "        menu,\n",
    "        mix_share,\n",
    "        csat,\n",
    "        pricing,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# True elasticities (hidden)\n",
    "# -----------------------------\n",
    "beta_gc = {\n",
    "    \"digital_promo\": 0.06,\n",
    "    \"nondigital_promo\": 0.03,\n",
    "    \"media\": 0.05,\n",
    "    \"menu\": 0.04,\n",
    "    \"mix_share\": 0.02,\n",
    "    \"csat\": 0.03,\n",
    "    \"pricing\": -0.08,\n",
    "}\n",
    "\n",
    "beta_ac = {\n",
    "    \"digital_promo\": 0.01,\n",
    "    \"nondigital_promo\": 0.015,\n",
    "    \"media\": 0.005,\n",
    "    \"menu\": 0.06,\n",
    "    \"mix_share\": 0.04,\n",
    "    \"csat\": 0.02,\n",
    "    \"pricing\": 0.12,\n",
    "}\n",
    "\n",
    "def linear_combination(X, beta_map):\n",
    "    out = np.zeros(n)\n",
    "    for domain, beta in beta_map.items():\n",
    "        cols = [c for c in X.columns if c.startswith(domain)]\n",
    "        out += beta * X[cols].mean(axis=1)\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Generate targets (log space)\n",
    "# -----------------------------\n",
    "log_gc = (\n",
    "    8.5\n",
    "    + linear_combination(X, beta_gc)\n",
    "    + store_effect_gc\n",
    "    + np.random.normal(0, 0.15, size=n)\n",
    ")\n",
    "\n",
    "log_ac = (\n",
    "    2.3\n",
    "    + linear_combination(X, beta_ac)\n",
    "    + store_effect_ac\n",
    "    + np.random.normal(0, 0.08, size=n)\n",
    ")\n",
    "\n",
    "panel[\"guest_counts\"] = np.exp(log_gc)\n",
    "panel[\"average_check\"] = np.exp(log_ac)\n",
    "\n",
    "# Identity preserved\n",
    "panel[\"sales\"] = panel[\"guest_counts\"] * panel[\"average_check\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Final dataset\n",
    "# -----------------------------\n",
    "df = pd.concat([panel, X], axis=1)\n",
    "\n",
    "# Optional: add logs & deltas for diagnostics\n",
    "df[\"log_gc\"] = np.log(df[\"guest_counts\"])\n",
    "df[\"log_ac\"] = np.log(df[\"average_check\"])\n",
    "df[\"log_sales\"] = np.log(df[\"sales\"])\n",
    "\n",
    "df = df.sort_values([\"store_id\", \"week_start\"])\n",
    "df[\"dlog_gc\"] = df.groupby(\"store_id\")[\"log_gc\"].diff()\n",
    "df[\"dlog_ac\"] = df.groupby(\"store_id\")[\"log_ac\"].diff()\n",
    "df[\"dlog_sales\"] = df.groupby(\"store_id\")[\"log_sales\"].diff()\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>week_start</th>\n",
       "      <th>guest_counts</th>\n",
       "      <th>average_check</th>\n",
       "      <th>sales</th>\n",
       "      <th>digital_promo_1</th>\n",
       "      <th>digital_promo_2</th>\n",
       "      <th>digital_promo_3</th>\n",
       "      <th>digital_promo_4</th>\n",
       "      <th>digital_promo_5</th>\n",
       "      <th>...</th>\n",
       "      <th>pricing_2</th>\n",
       "      <th>pricing_3</th>\n",
       "      <th>pricing_4</th>\n",
       "      <th>pricing_5</th>\n",
       "      <th>log_gc</th>\n",
       "      <th>log_ac</th>\n",
       "      <th>log_sales</th>\n",
       "      <th>dlog_gc</th>\n",
       "      <th>dlog_ac</th>\n",
       "      <th>dlog_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S01</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>3954.187562</td>\n",
       "      <td>10.615962</td>\n",
       "      <td>41977.506316</td>\n",
       "      <td>0.929579</td>\n",
       "      <td>1.162397</td>\n",
       "      <td>0.871167</td>\n",
       "      <td>1.354849</td>\n",
       "      <td>2.229055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746199</td>\n",
       "      <td>-0.006621</td>\n",
       "      <td>0.941221</td>\n",
       "      <td>1.135517</td>\n",
       "      <td>8.282530</td>\n",
       "      <td>2.362359</td>\n",
       "      <td>10.644889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S01</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>6265.926005</td>\n",
       "      <td>10.416270</td>\n",
       "      <td>65267.580205</td>\n",
       "      <td>-1.686452</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>-0.250716</td>\n",
       "      <td>-0.946267</td>\n",
       "      <td>0.725503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.796966</td>\n",
       "      <td>-0.263924</td>\n",
       "      <td>-0.454098</td>\n",
       "      <td>-0.572339</td>\n",
       "      <td>8.742882</td>\n",
       "      <td>2.343369</td>\n",
       "      <td>11.086251</td>\n",
       "      <td>0.460351</td>\n",
       "      <td>-0.018990</td>\n",
       "      <td>0.441362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S01</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>6016.031475</td>\n",
       "      <td>8.348131</td>\n",
       "      <td>50222.615937</td>\n",
       "      <td>0.285808</td>\n",
       "      <td>-0.162017</td>\n",
       "      <td>-0.455532</td>\n",
       "      <td>0.546936</td>\n",
       "      <td>1.500134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161102</td>\n",
       "      <td>-1.118125</td>\n",
       "      <td>0.025529</td>\n",
       "      <td>0.568790</td>\n",
       "      <td>8.702183</td>\n",
       "      <td>2.122038</td>\n",
       "      <td>10.824221</td>\n",
       "      <td>-0.040699</td>\n",
       "      <td>-0.221331</td>\n",
       "      <td>-0.262030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S01</td>\n",
       "      <td>2023-01-23</td>\n",
       "      <td>4401.770751</td>\n",
       "      <td>10.889535</td>\n",
       "      <td>47933.235085</td>\n",
       "      <td>-0.972781</td>\n",
       "      <td>-1.772048</td>\n",
       "      <td>-1.188044</td>\n",
       "      <td>-0.860905</td>\n",
       "      <td>-1.875745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.531658</td>\n",
       "      <td>0.630777</td>\n",
       "      <td>0.365515</td>\n",
       "      <td>1.003285</td>\n",
       "      <td>8.389762</td>\n",
       "      <td>2.387802</td>\n",
       "      <td>10.777564</td>\n",
       "      <td>-0.312421</td>\n",
       "      <td>0.265765</td>\n",
       "      <td>-0.046656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S01</td>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>5932.825495</td>\n",
       "      <td>9.177741</td>\n",
       "      <td>54449.936602</td>\n",
       "      <td>-1.725849</td>\n",
       "      <td>-1.121564</td>\n",
       "      <td>-0.398909</td>\n",
       "      <td>-1.518054</td>\n",
       "      <td>0.867727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.941251</td>\n",
       "      <td>-1.249387</td>\n",
       "      <td>-1.002577</td>\n",
       "      <td>-1.154580</td>\n",
       "      <td>8.688256</td>\n",
       "      <td>2.216781</td>\n",
       "      <td>10.905037</td>\n",
       "      <td>0.298494</td>\n",
       "      <td>-0.171021</td>\n",
       "      <td>0.127473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  store_id week_start  guest_counts  average_check         sales  \\\n",
       "0      S01 2023-01-02   3954.187562      10.615962  41977.506316   \n",
       "1      S01 2023-01-09   6265.926005      10.416270  65267.580205   \n",
       "2      S01 2023-01-16   6016.031475       8.348131  50222.615937   \n",
       "3      S01 2023-01-23   4401.770751      10.889535  47933.235085   \n",
       "4      S01 2023-01-30   5932.825495       9.177741  54449.936602   \n",
       "\n",
       "   digital_promo_1  digital_promo_2  digital_promo_3  digital_promo_4  \\\n",
       "0         0.929579         1.162397         0.871167         1.354849   \n",
       "1        -1.686452         0.002176        -0.250716        -0.946267   \n",
       "2         0.285808        -0.162017        -0.455532         0.546936   \n",
       "3        -0.972781        -1.772048        -1.188044        -0.860905   \n",
       "4        -1.725849        -1.121564        -0.398909        -1.518054   \n",
       "\n",
       "   digital_promo_5  ...  pricing_2  pricing_3  pricing_4  pricing_5    log_gc  \\\n",
       "0         2.229055  ...   0.746199  -0.006621   0.941221   1.135517  8.282530   \n",
       "1         0.725503  ...  -0.796966  -0.263924  -0.454098  -0.572339  8.742882   \n",
       "2         1.500134  ...   0.161102  -1.118125   0.025529   0.568790  8.702183   \n",
       "3        -1.875745  ...   1.531658   0.630777   0.365515   1.003285  8.389762   \n",
       "4         0.867727  ...  -0.941251  -1.249387  -1.002577  -1.154580  8.688256   \n",
       "\n",
       "     log_ac  log_sales   dlog_gc   dlog_ac  dlog_sales  \n",
       "0  2.362359  10.644889       NaN       NaN         NaN  \n",
       "1  2.343369  11.086251  0.460351 -0.018990    0.441362  \n",
       "2  2.122038  10.824221 -0.040699 -0.221331   -0.262030  \n",
       "3  2.387802  10.777564 -0.312421  0.265765   -0.046656  \n",
       "4  2.216781  10.905037  0.298494 -0.171021    0.127473  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def infer_feature_blocks(df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "    prefixes = [\n",
    "        \"digital_promo\", \"nondigital_promo\", \"media\", \"menu\",\n",
    "        \"mix_share\", \"csat\", \"pricing\"\n",
    "    ]\n",
    "    blocks = {}\n",
    "    for p in prefixes:\n",
    "        blocks[p] = [c for c in df.columns if c.startswith(p + \"_\")]\n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rolling_splits(\n",
    "    df: pd.DataFrame,\n",
    "    train_weeks: int = 104,\n",
    "    test_weeks: int = 13,\n",
    "    step_weeks: int = 13\n",
    ") -> List[Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp, pd.Timestamp]]:\n",
    "    weeks = sorted(df[\"week_start\"].unique())\n",
    "    splits = []\n",
    "    for start_idx in range(0, len(weeks) - train_weeks - test_weeks + 1, step_weeks):\n",
    "        train_start = weeks[start_idx]\n",
    "        train_end   = weeks[start_idx + train_weeks - 1]\n",
    "        test_start  = weeks[start_idx + train_weeks]\n",
    "        test_end    = weeks[start_idx + train_weeks + test_weeks - 1]\n",
    "        splits.append((train_start, train_end, test_start, test_end))\n",
    "    return splits\n",
    "\n",
    "def make_yoy_holdout_splits(\n",
    "    df: pd.DataFrame,\n",
    "    train_end_min: str = \"2024-12-30\",\n",
    "    test_weeks: int = 13,\n",
    "    step_weeks: int = 13\n",
    ") -> List[Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp, pd.Timestamp]]:\n",
    "    weeks = sorted(df[\"week_start\"].unique())\n",
    "    min_end = pd.Timestamp(train_end_min)\n",
    "    splits = []\n",
    "    for train_end in weeks:\n",
    "        if train_end < min_end:\n",
    "            continue\n",
    "        test_start = train_end + pd.Timedelta(weeks=52)\n",
    "        test_end   = test_start + pd.Timedelta(weeks=test_weeks - 1)\n",
    "        if test_end > weeks[-1]:\n",
    "            break\n",
    "        train_start = weeks[0]\n",
    "        splits.append((train_start, train_end, test_start, test_end))\n",
    "    # thin by step\n",
    "    return splits[::max(1, step_weeks // 13)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def within_transform(\n",
    "    df_sub: pd.DataFrame,\n",
    "    y_col: str,\n",
    "    x_cols: List[str],\n",
    "    entity_col: str = \"store_id\"\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Demean y and X within entity (store).\n",
    "    Returns numpy arrays ready for sklearn.\n",
    "    \"\"\"\n",
    "    g = df_sub.groupby(entity_col)\n",
    "    y = df_sub[y_col] - g[y_col].transform(\"mean\")\n",
    "    X = df_sub[x_cols].copy()\n",
    "    for c in x_cols:\n",
    "        X[c] = X[c] - g[c].transform(\"mean\")\n",
    "    return y.values, X.values, x_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mundlak_terms(\n",
    "    df_sub: pd.DataFrame,\n",
    "    x_cols: List[str],\n",
    "    entity_col: str = \"store_id\",\n",
    "    suffix: str = \"_mean_store\"\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    g = df_sub.groupby(entity_col)\n",
    "    mundlak = {}\n",
    "    for c in x_cols:\n",
    "        mundlak[c + suffix] = g[c].transform(\"mean\")\n",
    "    df_aug = df_sub.copy()\n",
    "    for k, v in mundlak.items():\n",
    "        df_aug[k] = v\n",
    "    x_aug_cols = x_cols + list(mundlak.keys())\n",
    "    return df_aug, x_aug_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "@dataclass\n",
    "class FitResult:\n",
    "    coef: pd.Series\n",
    "    se: pd.Series\n",
    "    pval: pd.Series\n",
    "    metrics: Dict[str, float]\n",
    "\n",
    "def fit_ols_sm(y: np.ndarray, X: np.ndarray, colnames: List[str]) -> FitResult:\n",
    "    Xc = sm.add_constant(X, has_constant=\"add\")\n",
    "    model = sm.OLS(y, Xc).fit()\n",
    "    params = pd.Series(model.params[1:], index=colnames)  # drop const\n",
    "    se = pd.Series(model.bse[1:], index=colnames)\n",
    "    pval = pd.Series(model.pvalues[1:], index=colnames)\n",
    "\n",
    "    yhat = model.predict(Xc)\n",
    "    metrics = {\n",
    "        \"r2\": float(model.rsquared),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y, yhat))),\n",
    "        \"mae\": float(mean_absolute_error(y, yhat)),\n",
    "    }\n",
    "    return FitResult(params, se, pval, metrics)\n",
    "\n",
    "def fit_ridge(y: np.ndarray, X: np.ndarray, colnames: List[str], alpha: float) -> FitResult:\n",
    "    model = Ridge(alpha=alpha, fit_intercept=True, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    yhat = model.predict(X)\n",
    "    coef = pd.Series(model.coef_, index=colnames)\n",
    "    se = pd.Series(np.nan, index=colnames)\n",
    "    pval = pd.Series(np.nan, index=colnames)\n",
    "    metrics = {\n",
    "        \"r2\": float(r2_score(y, yhat)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y, yhat))),\n",
    "        \"mae\": float(mean_absolute_error(y, yhat)),\n",
    "    }\n",
    "    return FitResult(coef, se, pval, metrics)\n",
    "\n",
    "def fit_elasticnet(y: np.ndarray, X: np.ndarray, colnames: List[str], alpha: float, l1_ratio: float) -> FitResult:\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, fit_intercept=True, random_state=42, max_iter=10000)\n",
    "    model.fit(X, y)\n",
    "    yhat = model.predict(X)\n",
    "    coef = pd.Series(model.coef_, index=colnames)\n",
    "    se = pd.Series(np.nan, index=colnames)\n",
    "    pval = pd.Series(np.nan, index=colnames)\n",
    "    metrics = {\n",
    "        \"r2\": float(r2_score(y, yhat)),\n",
    "        \"rmse\": float(np.sqrt(mean_squared_error(y, yhat))),\n",
    "        \"mae\": float(mean_absolute_error(y, yhat)),\n",
    "    }\n",
    "    return FitResult(coef, se, pval, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "\n",
    "def compute_coef_table(fit: FitResult) -> pd.DataFrame:\n",
    "    dfc = pd.DataFrame({\n",
    "        \"feature\": fit.coef.index,\n",
    "        \"coef\": fit.coef.values,\n",
    "        \"se\": fit.se.values,\n",
    "        \"pval\": fit.pval.values,\n",
    "    })\n",
    "    dfc[\"sign\"] = np.sign(dfc[\"coef\"]).astype(int)\n",
    "    dfc[\"abs_coef\"] = np.abs(dfc[\"coef\"])\n",
    "    dfc[\"rank_abs\"] = dfc[\"abs_coef\"].rank(ascending=False, method=\"average\")\n",
    "    # \"selected\" for regularization: non-zero coefficient (or abs > eps)\n",
    "    eps = 1e-10\n",
    "    dfc[\"selected_flag\"] = (dfc[\"abs_coef\"] > eps).astype(int)\n",
    "    return dfc.sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "def run_one_experiment(\n",
    "    df: pd.DataFrame,\n",
    "    y_col: str,\n",
    "    feature_cols: List[str],\n",
    "    panel_method: str,          # \"fe\" or \"mundlak\"\n",
    "    algorithm: str,             # \"ols\" | \"ridge\" | \"elasticnet\"\n",
    "    algo_params: Dict,\n",
    "    split: Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp, pd.Timestamp],\n",
    "    entity_col: str = \"store_id\",\n",
    "    run_name: Optional[str] = None,\n",
    ") -> Dict:\n",
    "    train_start, train_end, test_start, test_end = split\n",
    "\n",
    "    df_train = df[(df[\"week_start\"] >= train_start) & (df[\"week_start\"] <= train_end)].copy()\n",
    "    df_test  = df[(df[\"week_start\"] >= test_start) & (df[\"week_start\"] <= test_end)].copy()\n",
    "\n",
    "    # Build design matrix based on panel method\n",
    "    if panel_method == \"fe\":\n",
    "        y_tr, X_tr, cols_used = within_transform(df_train, y_col, feature_cols, entity_col=entity_col)\n",
    "\n",
    "    elif panel_method == \"mundlak\":\n",
    "        df_aug, cols_used = add_mundlak_terms(df_train, feature_cols, entity_col=entity_col)\n",
    "        y_tr = df_aug[y_col].values\n",
    "        X_tr = df_aug[cols_used].values\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown panel_method: {panel_method}\")\n",
    "\n",
    "    # Fit\n",
    "    if algorithm == \"ols\":\n",
    "        fit = fit_ols_sm(y_tr, X_tr, cols_used)\n",
    "    elif algorithm == \"ridge\":\n",
    "        fit = fit_ridge(y_tr, X_tr, cols_used, alpha=float(algo_params[\"alpha\"]))\n",
    "    elif algorithm == \"elasticnet\":\n",
    "        fit = fit_elasticnet(\n",
    "            y_tr, X_tr, cols_used,\n",
    "            alpha=float(algo_params[\"alpha\"]),\n",
    "            l1_ratio=float(algo_params[\"l1_ratio\"])\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "\n",
    "    coef_table = compute_coef_table(fit)\n",
    "\n",
    "    # MLflow logging\n",
    "    with mlflow.start_run(run_name=run_name, nested=True):\n",
    "        # Params (experiment axes)\n",
    "        mlflow.log_param(\"target\", y_col)\n",
    "        mlflow.log_param(\"panel_method\", panel_method)\n",
    "        mlflow.log_param(\"algorithm\", algorithm)\n",
    "        mlflow.log_param(\"train_start\", str(train_start.date()))\n",
    "        mlflow.log_param(\"train_end\", str(train_end.date()))\n",
    "        mlflow.log_param(\"test_start\", str(test_start.date()))\n",
    "        mlflow.log_param(\"test_end\", str(test_end.date()))\n",
    "        mlflow.log_param(\"n_features\", len(feature_cols))\n",
    "        mlflow.log_param(\"n_obs_train\", len(df_train))\n",
    "\n",
    "        for k, v in algo_params.items():\n",
    "            mlflow.log_param(f\"hp_{k}\", v)\n",
    "\n",
    "        # Metrics (sanity only)\n",
    "        for k, v in fit.metrics.items():\n",
    "            mlflow.log_metric(k, v)\n",
    "\n",
    "        # Artifacts: feature list + coefficient table\n",
    "        mlflow.log_text(\"\\n\".join(feature_cols), artifact_file=\"features_used.txt\")\n",
    "        mlflow.log_text(json.dumps(algo_params, indent=2), artifact_file=\"algo_params.json\")\n",
    "        mlflow.log_text(coef_table.to_csv(index=False), artifact_file=\"coef_table.csv\")\n",
    "\n",
    "    # Standardized record for your survival aggregation later\n",
    "    return {\n",
    "        \"target\": y_col,\n",
    "        \"panel_method\": panel_method,\n",
    "        \"algorithm\": algorithm,\n",
    "        \"algo_params\": algo_params,\n",
    "        \"train_start\": train_start,\n",
    "        \"train_end\": train_end,\n",
    "        \"test_start\": test_start,\n",
    "        \"test_end\": test_end,\n",
    "        \"coef_table\": coef_table,     # keep in-memory for aggregation\n",
    "        \"fit_metrics\": fit.metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrate_experiments(\n",
    "    df: pd.DataFrame,\n",
    "    targets: List[str],\n",
    "    blocks: Dict[str, List[str]],\n",
    "    panel_methods: List[str],\n",
    "    algos: List[Tuple[str, Dict]],\n",
    "    splits: List[Tuple[pd.Timestamp, pd.Timestamp, pd.Timestamp, pd.Timestamp]],\n",
    "    block_sets: Optional[Dict[str, List[str]]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    block_sets lets you define feature subsets like:\n",
    "      - single domain\n",
    "      - domain combos (promo + pricing)\n",
    "      - full model set\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    if block_sets is None:\n",
    "        # default: single-domain experiments\n",
    "        block_sets = {k: v for k, v in blocks.items()}\n",
    "\n",
    "    with mlflow.start_run(run_name=\"layer2_regression_experiments\"):\n",
    "        mlflow.log_param(\"n_splits\", len(splits))\n",
    "        mlflow.log_param(\"n_block_sets\", len(block_sets))\n",
    "        mlflow.log_param(\"targets\", \",\".join(targets))\n",
    "\n",
    "        for target in targets:\n",
    "            for block_name, feature_cols in block_sets.items():\n",
    "                for panel_method in panel_methods:\n",
    "                    for algo_name, algo_params in algos:\n",
    "                        for s_idx, split in enumerate(splits):\n",
    "                            run_name = f\"{target}__{block_name}__{panel_method}__{algo_name}__split{s_idx:02d}\"\n",
    "                            out = run_one_experiment(\n",
    "                                df=df,\n",
    "                                y_col=target,\n",
    "                                feature_cols=feature_cols,\n",
    "                                panel_method=panel_method,\n",
    "                                algorithm=algo_name,\n",
    "                                algo_params=algo_params,\n",
    "                                split=split,\n",
    "                                run_name=run_name\n",
    "                            )\n",
    "                            out[\"block_name\"] = block_name\n",
    "                            out[\"split_idx\"] = s_idx\n",
    "                            results.append(out)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = infer_feature_blocks(df)\n",
    "\n",
    "# targets: primary log-level first\n",
    "targets = [\"log_gc\", \"log_ac\"]\n",
    "\n",
    "panel_methods = [\"fe\", \"mundlak\"]\n",
    "\n",
    "algos = [\n",
    "    (\"ols\", {}),\n",
    "    (\"ridge\", {\"alpha\": 1.0}),\n",
    "    (\"ridge\", {\"alpha\": 10.0}),\n",
    "    (\"elasticnet\", {\"alpha\": 0.1, \"l1_ratio\": 0.2}),\n",
    "    (\"elasticnet\", {\"alpha\": 0.1, \"l1_ratio\": 0.8}),\n",
    "]\n",
    "\n",
    "splits = make_rolling_splits(df, train_weeks=104, test_weeks=13, step_weeks=13)\n",
    "\n",
    "results = orchestrate_experiments(\n",
    "    df=df,\n",
    "    targets=targets,\n",
    "    blocks=blocks,\n",
    "    panel_methods=panel_methods,\n",
    "    algos=algos,\n",
    "    splits=splits\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yoy_contributions_from_coef_table(\n",
    "    df: pd.DataFrame,\n",
    "    coef_table: pd.DataFrame,\n",
    "    feature_prefix_filter: Optional[str] = None,\n",
    "    week_t: Optional[pd.Timestamp] = None,\n",
    "    entity_col: str = \"store_id\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Produces YoY contribution per feature at aggregate level:\n",
    "      (mean x_t - mean x_t-52) * beta\n",
    "    \"\"\"\n",
    "    ct = coef_table[[\"feature\", \"coef\"]].copy()\n",
    "    if feature_prefix_filter:\n",
    "        ct = ct[ct[\"feature\"].str.startswith(feature_prefix_filter)]\n",
    "\n",
    "    if week_t is None:\n",
    "        week_t = df[\"week_start\"].max()\n",
    "    week_tm52 = week_t - pd.Timedelta(weeks=52)\n",
    "\n",
    "    df_t = df[df[\"week_start\"] == week_t]\n",
    "    df_b = df[df[\"week_start\"] == week_tm52]\n",
    "\n",
    "    rows = []\n",
    "    for _, r in ct.iterrows():\n",
    "        f = r[\"feature\"]\n",
    "        beta = r[\"coef\"]\n",
    "        if f not in df.columns:\n",
    "            continue\n",
    "        dx = df_t[f].mean() - df_b[f].mean()\n",
    "        rows.append({\n",
    "            \"feature\": f,\n",
    "            \"beta\": beta,\n",
    "            \"dx_yoy\": dx,\n",
    "            \"contrib_yoy\": dx * beta\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\"contrib_yoy\", ascending=False)\n",
    "    out[\"abs_contrib_yoy\"] = out[\"contrib_yoy\"].abs()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you pick one GC run and one AC run (same feature set preferred)\n",
    "gc_contrib = yoy_contributions_from_coef_table(df, gc_run[\"coef_table\"], week_t=df[\"week_start\"].max())\n",
    "ac_contrib = yoy_contributions_from_coef_table(df, ac_run[\"coef_table\"], week_t=df[\"week_start\"].max())\n",
    "\n",
    "sales_contrib = (\n",
    "    gc_contrib[[\"feature\", \"contrib_yoy\"]].rename(columns={\"contrib_yoy\":\"gc_contrib_yoy\"})\n",
    "    .merge(ac_contrib[[\"feature\", \"contrib_yoy\"]].rename(columns={\"contrib_yoy\":\"ac_contrib_yoy\"}),\n",
    "           on=\"feature\", how=\"outer\")\n",
    "    .fillna(0.0)\n",
    ")\n",
    "sales_contrib[\"sales_contrib_yoy\"] = sales_contrib[\"gc_contrib_yoy\"] + sales_contrib[\"ac_contrib_yoy\"]\n",
    "sales_contrib = sales_contrib.sort_values(\"sales_contrib_yoy\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def infer_domain_from_feature(feature: str) -> str:\n",
    "    # Matches your synthetic prefixes; replace with your feature_catalog mapping if you have it\n",
    "    for d in [\"digital_promo\", \"nondigital_promo\", \"media\", \"menu\", \"mix_share\", \"csat\", \"pricing\"]:\n",
    "        if feature.startswith(d + \"_\"):\n",
    "            return d\n",
    "    return \"other\"\n",
    "\n",
    "def results_to_long_table(results: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    results: list of outputs from run_one_experiment / orchestrate_experiments\n",
    "    Returns: long table with one row per feature per run\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for r in results:\n",
    "        ct = r[\"coef_table\"].copy()\n",
    "        ct[\"target\"] = r[\"target\"]\n",
    "        ct[\"block_name\"] = r.get(\"block_name\", None)\n",
    "        ct[\"panel_method\"] = r[\"panel_method\"]\n",
    "        ct[\"algorithm\"] = r[\"algorithm\"]\n",
    "        ct[\"split_idx\"] = r.get(\"split_idx\", None)\n",
    "\n",
    "        # For Mundlak runs: separate within X vs store-mean X if you want\n",
    "        ct[\"is_mundlak_mean_term\"] = ct[\"feature\"].str.endswith(\"_mean_store\")\n",
    "\n",
    "        # Standard identifiers\n",
    "        ct[\"run_key\"] = (\n",
    "            ct[\"target\"].astype(str) + \"|\" +\n",
    "            ct[\"block_name\"].astype(str) + \"|\" +\n",
    "            ct[\"panel_method\"].astype(str) + \"|\" +\n",
    "            ct[\"algorithm\"].astype(str) + \"|split\" +\n",
    "            ct[\"split_idx\"].astype(str)\n",
    "        )\n",
    "\n",
    "        # Derived fields\n",
    "        ct[\"domain\"] = ct[\"feature\"].map(infer_domain_from_feature)\n",
    "\n",
    "        rows.append(ct)\n",
    "\n",
    "    long_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # helpful flags\n",
    "    long_df[\"has_pval\"] = long_df[\"pval\"].notna()\n",
    "    long_df[\"sig_05\"] = (long_df[\"pval\"] < 0.05).astype(int)\n",
    "    long_df[\"sig_10\"] = (long_df[\"pval\"] < 0.10).astype(int)\n",
    "\n",
    "    # protect against tiny denom in CV\n",
    "    long_df[\"abs_coef\"] = long_df[\"coef\"].abs()\n",
    "\n",
    "    return long_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stability_metrics(\n",
    "    long_df: pd.DataFrame,\n",
    "    selected_only_for_stats: bool = True,\n",
    "    significance_level: float = 0.05\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns one row per feature with stability metrics.\n",
    "    \"\"\"\n",
    "    df = long_df.copy()\n",
    "\n",
    "    # Optionally compute stats only where selected_flag==1\n",
    "    if selected_only_for_stats:\n",
    "        df_stats = df[df[\"selected_flag\"] == 1].copy()\n",
    "    else:\n",
    "        df_stats = df.copy()\n",
    "\n",
    "    sig_col = \"sig_05\" if significance_level == 0.05 else \"sig_10\"\n",
    "\n",
    "    # Denominators\n",
    "    total_runs_per_feature = df.groupby(\"feature\")[\"run_key\"].nunique().rename(\"n_runs_total\")\n",
    "    selected_runs_per_feature = df[df[\"selected_flag\"] == 1].groupby(\"feature\")[\"run_key\"].nunique().rename(\"n_runs_selected\")\n",
    "\n",
    "    # Core stats (computed on df_stats)\n",
    "    g = df_stats.groupby(\"feature\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"domain\": g[\"domain\"].first(),\n",
    "        \"n_rows_used_for_stats\": g.size(),\n",
    "        \"coef_mean\": g[\"coef\"].mean(),\n",
    "        \"coef_median\": g[\"coef\"].median(),\n",
    "        \"coef_std\": g[\"coef\"].std(ddof=0),\n",
    "        \"abs_coef_mean\": g[\"abs_coef\"].mean(),\n",
    "        \"rank_abs_median\": g[\"rank_abs\"].median(),\n",
    "        \"sign_mode\": g[\"sign\"].agg(lambda s: int(np.sign(s.sum())) if len(s) else 0),\n",
    "        \"sign_consistency\": g[\"sign\"].agg(lambda s: np.nan if len(s) == 0 else (s.eq(s.mode().iloc[0]).mean())),\n",
    "    }).reset_index()\n",
    "\n",
    "    # CV = std/|mean|\n",
    "    eps = 1e-12\n",
    "    out[\"coef_cv\"] = out[\"coef_std\"] / (out[\"coef_mean\"].abs() + eps)\n",
    "\n",
    "    # Selection rates\n",
    "    out = out.merge(total_runs_per_feature.reset_index(), on=\"feature\", how=\"left\")\n",
    "    out = out.merge(selected_runs_per_feature.reset_index(), on=\"feature\", how=\"left\")\n",
    "    out[\"n_runs_selected\"] = out[\"n_runs_selected\"].fillna(0).astype(int)\n",
    "    out[\"pct_selected\"] = out[\"n_runs_selected\"] / out[\"n_runs_total\"]\n",
    "\n",
    "    # Sign consistency conditional on selected (recommended)\n",
    "    # (If df_stats already selected-only, this is already conditional.)\n",
    "    # For transparency:\n",
    "    out[\"sign_consistency_cond_selected\"] = out[\"sign_consistency\"]\n",
    "\n",
    "    # Significance: only on rows where p-values exist (OLS runs)\n",
    "    df_sig_base = df_stats[df_stats[\"has_pval\"]].copy()\n",
    "    if not df_sig_base.empty:\n",
    "        sig_rate = df_sig_base.groupby(\"feature\")[sig_col].mean().rename(f\"pct_significant_{int(significance_level*100)}\")\n",
    "        out = out.merge(sig_rate.reset_index(), on=\"feature\", how=\"left\")\n",
    "    else:\n",
    "        out[f\"pct_significant_{int(significance_level*100)}\"] = np.nan\n",
    "\n",
    "    # Also helpful: number of distinct “worlds” (splits) survived\n",
    "    survived_splits = df[df[\"selected_flag\"] == 1].groupby(\"feature\")[\"split_idx\"].nunique().rename(\"n_splits_selected\")\n",
    "    out = out.merge(survived_splits.reset_index(), on=\"feature\", how=\"left\")\n",
    "    out[\"n_splits_selected\"] = out[\"n_splits_selected\"].fillna(0).astype(int)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_selection_rules(\n",
    "    stability_df: pd.DataFrame,\n",
    "    min_pct_selected: float = 0.60,\n",
    "    min_sign_consistency: float = 0.80,\n",
    "    max_coef_cv: float = 1.00,\n",
    "    max_median_rank: float = 20,\n",
    "    min_pct_sig_5: float = 0.50,\n",
    "    require_significance_if_available: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    df = stability_df.copy()\n",
    "\n",
    "    # Base filters\n",
    "    keep = (\n",
    "        (df[\"pct_selected\"] >= min_pct_selected) &\n",
    "        (df[\"sign_consistency_cond_selected\"] >= min_sign_consistency) &\n",
    "        (df[\"coef_cv\"] <= max_coef_cv) &\n",
    "        (df[\"rank_abs_median\"] <= max_median_rank)\n",
    "    )\n",
    "\n",
    "    if require_significance_if_available:\n",
    "        sig_col = \"pct_significant_5\"\n",
    "        if sig_col in df.columns:\n",
    "            keep = keep & (df[sig_col].isna() | (df[sig_col] >= min_pct_sig_5))\n",
    "\n",
    "    df[\"pass_rules\"] = keep.astype(int)\n",
    "\n",
    "    # A useful composite score for sorting\n",
    "    df[\"stability_score\"] = (\n",
    "        2.0 * df[\"pct_selected\"].fillna(0) +\n",
    "        1.5 * df[\"sign_consistency_cond_selected\"].fillna(0) +\n",
    "        1.0 * (1.0 / (1.0 + df[\"coef_cv\"].fillna(10))) +\n",
    "        0.5 * (1.0 / (1.0 + df[\"rank_abs_median\"].fillna(999)))\n",
    "    )\n",
    "\n",
    "    return df.sort_values([\"pass_rules\", \"stability_score\"], ascending=[False, False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_domain_coverage(\n",
    "    ranked_df: pd.DataFrame,\n",
    "    final_k: int = 12,\n",
    "    min_per_domain: dict = None,\n",
    "    max_per_domain: dict = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ranked_df: output of apply_selection_rules() (already sorted)\n",
    "    \"\"\"\n",
    "    if min_per_domain is None:\n",
    "        min_per_domain = {\n",
    "            \"digital_promo\": 1,\n",
    "            \"nondigital_promo\": 1,\n",
    "            \"media\": 1,\n",
    "            \"menu\": 1,\n",
    "            \"pricing\": 1,\n",
    "            \"csat\": 1,\n",
    "            # mix_share often capped rather than known-min\n",
    "        }\n",
    "    if max_per_domain is None:\n",
    "        max_per_domain = {\n",
    "            \"mix_share\": 2,\n",
    "            \"media\": 2,\n",
    "            \"digital_promo\": 2,\n",
    "            \"nondigital_promo\": 2,\n",
    "            \"menu\": 2,\n",
    "            \"pricing\": 2,\n",
    "            \"csat\": 2,\n",
    "            \"other\": 1\n",
    "        }\n",
    "\n",
    "    chosen = []\n",
    "    counts = {}\n",
    "\n",
    "    # First: satisfy minimums using best-ranked per domain\n",
    "    for dom, m in min_per_domain.items():\n",
    "        cand = ranked_df[(ranked_df[\"domain\"] == dom) & (ranked_df[\"pass_rules\"] == 1)]\n",
    "        for _, row in cand.head(m).iterrows():\n",
    "            f = row[\"feature\"]\n",
    "            if f in chosen:\n",
    "                continue\n",
    "            chosen.append(f)\n",
    "            counts[dom] = counts.get(dom, 0) + 1\n",
    "\n",
    "    # Then: fill remaining slots by overall rank, respecting max caps\n",
    "    for _, row in ranked_df.iterrows():\n",
    "        if len(chosen) >= final_k:\n",
    "            break\n",
    "        if row[\"pass_rules\"] != 1:\n",
    "            continue\n",
    "        dom = row[\"domain\"]\n",
    "        f = row[\"feature\"]\n",
    "        if f in chosen:\n",
    "            continue\n",
    "        if counts.get(dom, 0) >= max_per_domain.get(dom, 999):\n",
    "            continue\n",
    "        chosen.append(f)\n",
    "        counts[dom] = counts.get(dom, 0) + 1\n",
    "\n",
    "    out = ranked_df[ranked_df[\"feature\"].isin(chosen)].copy()\n",
    "    out[\"selected_final\"] = 1\n",
    "    # preserve chosen order\n",
    "    out[\"selected_order\"] = out[\"feature\"].map({f:i for i,f in enumerate(chosen, start=1)})\n",
    "    return out.sort_values(\"selected_order\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) build long table\n",
    "long_df = results_to_long_table(results)\n",
    "\n",
    "# (optional) exclude Mundlak mean terms if you only care about within X\n",
    "# long_df = long_df[long_df[\"is_mundlak_mean_term\"] == False].copy()\n",
    "\n",
    "# 2) stability metrics\n",
    "stab = compute_stability_metrics(long_df, selected_only_for_stats=True, significance_level=0.05)\n",
    "\n",
    "# 3) apply rules + rank\n",
    "ranked = apply_selection_rules(\n",
    "    stab,\n",
    "    min_pct_selected=0.60,\n",
    "    min_sign_consistency=0.80,\n",
    "    max_coef_cv=1.00,\n",
    "    max_median_rank=20,\n",
    "    require_significance_if_available=False\n",
    ")\n",
    "\n",
    "# 4) final driver list (8–15)\n",
    "final = enforce_domain_coverage(ranked, final_k=12)\n",
    "\n",
    "final[[\n",
    "    \"feature\",\"domain\",\"pct_selected\",\"sign_consistency_cond_selected\",\n",
    "    \"coef_mean\",\"coef_cv\",\"rank_abs_median\",\"stability_score\"\n",
    "]].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_champion_run(results, target, block_name=None, panel_method=\"fe\", algorithm=\"ridge\", hp_filter=None):\n",
    "    \"\"\"\n",
    "    Picks the run with the highest sanity metric (r2) among those matching the spec.\n",
    "    hp_filter: dict like {\"alpha\": 10.0} to enforce specific hp\n",
    "    \"\"\"\n",
    "    cand = []\n",
    "    for r in results:\n",
    "        if r[\"target\"] != target:\n",
    "            continue\n",
    "        if block_name is not None and r.get(\"block_name\") != block_name:\n",
    "            continue\n",
    "        if r[\"panel_method\"] != panel_method:\n",
    "            continue\n",
    "        if r[\"algorithm\"] != algorithm:\n",
    "            continue\n",
    "        if hp_filter:\n",
    "            ok = True\n",
    "            for k, v in hp_filter.items():\n",
    "                if str(r[\"algo_params\"].get(k)) != str(v):\n",
    "                    ok = False\n",
    "                    break\n",
    "            if not ok:\n",
    "                continue\n",
    "        cand.append(r)\n",
    "    if not cand:\n",
    "        raise ValueError(\"No runs match the champion selection criteria.\")\n",
    "    # sanity metric only\n",
    "    cand = sorted(cand, key=lambda x: x[\"fit_metrics\"].get(\"r2\", -np.inf), reverse=True)\n",
    "    return cand[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_champion_on_full_window(\n",
    "    df, y_col, feature_cols,\n",
    "    panel_method=\"fe\",\n",
    "    algorithm=\"ridge\",\n",
    "    algo_params=None,\n",
    "    entity_col=\"store_id\",\n",
    "    train_start=\"2023-01-02\",\n",
    "    train_end=\"2025-12-29\"\n",
    "):\n",
    "    if algo_params is None:\n",
    "        algo_params = {\"alpha\": 10.0} if algorithm == \"ridge\" else {}\n",
    "\n",
    "    df_train = df[(df[\"week_start\"] >= pd.Timestamp(train_start)) &\n",
    "                  (df[\"week_start\"] <= pd.Timestamp(train_end))].copy()\n",
    "\n",
    "    if panel_method == \"fe\":\n",
    "        y_tr, X_tr, cols_used = within_transform(df_train, y_col, feature_cols, entity_col=entity_col)\n",
    "    elif panel_method == \"mundlak\":\n",
    "        df_aug, cols_used = add_mundlak_terms(df_train, feature_cols, entity_col=entity_col)\n",
    "        y_tr = df_aug[y_col].values\n",
    "        X_tr = df_aug[cols_used].values\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown panel_method: {panel_method}\")\n",
    "\n",
    "    if algorithm == \"ols\":\n",
    "        fit = fit_ols_sm(y_tr, X_tr, cols_used)\n",
    "    elif algorithm == \"ridge\":\n",
    "        fit = fit_ridge(y_tr, X_tr, cols_used, alpha=float(algo_params[\"alpha\"]))\n",
    "    elif algorithm == \"elasticnet\":\n",
    "        fit = fit_elasticnet(y_tr, X_tr, cols_used,\n",
    "                             alpha=float(algo_params[\"alpha\"]),\n",
    "                             l1_ratio=float(algo_params[\"l1_ratio\"]))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
    "\n",
    "    coef_table = compute_coef_table(fit)\n",
    "    return fit, coef_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yoy_driver_contrib(\n",
    "    df: pd.DataFrame,\n",
    "    coef_table: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    week_t: pd.Timestamp,\n",
    "    yoy_lag_weeks: int = 52\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns YoY contribution per feature at aggregate level:\n",
    "      (mean(x_t) - mean(x_t-lag)) * beta\n",
    "    \"\"\"\n",
    "    week_tm = week_t - pd.Timedelta(weeks=yoy_lag_weeks)\n",
    "\n",
    "    df_t = df[df[\"week_start\"] == week_t]\n",
    "    df_m = df[df[\"week_start\"] == week_tm]\n",
    "\n",
    "    beta = coef_table.set_index(\"feature\")[\"coef\"].reindex(feature_cols)\n",
    "\n",
    "    rows = []\n",
    "    for f in feature_cols:\n",
    "        if f not in df.columns:\n",
    "            continue\n",
    "        dx = df_t[f].mean() - df_m[f].mean()\n",
    "        b = float(beta.loc[f]) if pd.notna(beta.loc[f]) else 0.0\n",
    "        rows.append({\"feature\": f, \"dx_yoy\": dx, \"beta\": b, \"contrib_yoy\": dx * b})\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    out[\"abs_contrib_yoy\"] = out[\"contrib_yoy\"].abs()\n",
    "    return out.sort_values(\"contrib_yoy\", ascending=False)\n",
    "\n",
    "def yoy_target_actual(df, log_target_col, week_t, lag_weeks=52):\n",
    "    week_tm = week_t - pd.Timedelta(weeks=lag_weeks)\n",
    "    y_t = df[df[\"week_start\"] == week_t][log_target_col].mean()\n",
    "    y_m = df[df[\"week_start\"] == week_tm][log_target_col].mean()\n",
    "    return float(y_t - y_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_sales_yoy(\n",
    "    df: pd.DataFrame,\n",
    "    week_t: pd.Timestamp,\n",
    "    final_features: list,\n",
    "    gc_coef_table: pd.DataFrame,\n",
    "    ac_coef_table: pd.DataFrame\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - driver table (GC/AC/Sales contributions)\n",
    "      - totals + residuals vs actual YoY deltas\n",
    "    \"\"\"\n",
    "    gc = yoy_driver_contrib(df, gc_coef_table, final_features, week_t)\n",
    "    ac = yoy_driver_contrib(df, ac_coef_table, final_features, week_t)\n",
    "\n",
    "    driver = (\n",
    "        gc[[\"feature\", \"contrib_yoy\"]].rename(columns={\"contrib_yoy\": \"gc_contrib_yoy\"})\n",
    "        .merge(ac[[\"feature\", \"contrib_yoy\"]].rename(columns={\"contrib_yoy\": \"ac_contrib_yoy\"}),\n",
    "               on=\"feature\", how=\"outer\")\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    driver[\"sales_contrib_yoy\"] = driver[\"gc_contrib_yoy\"] + driver[\"ac_contrib_yoy\"]\n",
    "    driver[\"abs_sales_contrib_yoy\"] = driver[\"sales_contrib_yoy\"].abs()\n",
    "    driver = driver.sort_values(\"sales_contrib_yoy\", ascending=False)\n",
    "\n",
    "    # Totals from model contributions\n",
    "    total_gc = float(driver[\"gc_contrib_yoy\"].sum())\n",
    "    total_ac = float(driver[\"ac_contrib_yoy\"].sum())\n",
    "    total_sales_from_models = float(driver[\"sales_contrib_yoy\"].sum())\n",
    "\n",
    "    # Actual YoY deltas (aggregate)\n",
    "    actual_gc = yoy_target_actual(df, \"log_gc\", week_t)\n",
    "    actual_ac = yoy_target_actual(df, \"log_ac\", week_t)\n",
    "    actual_sales = yoy_target_actual(df, \"log_sales\", week_t)\n",
    "\n",
    "    # Identity checks\n",
    "    identity_actual = actual_gc + actual_ac\n",
    "    identity_model = total_gc + total_ac\n",
    "\n",
    "    return {\n",
    "        \"driver_table\": driver.reset_index(drop=True),\n",
    "        \"totals\": {\n",
    "            \"week_t\": str(week_t.date()),\n",
    "            \"actual_yoy_log_gc\": actual_gc,\n",
    "            \"actual_yoy_log_ac\": actual_ac,\n",
    "            \"actual_yoy_log_sales\": actual_sales,\n",
    "            \"actual_identity_check_gc_plus_ac\": identity_actual,\n",
    "\n",
    "            \"modeled_yoy_log_gc_from_drivers\": total_gc,\n",
    "            \"modeled_yoy_log_ac_from_drivers\": total_ac,\n",
    "            \"modeled_yoy_log_sales_from_drivers\": total_sales_from_models,\n",
    "            \"modeled_identity_check_gc_plus_ac\": identity_model,\n",
    "\n",
    "            \"residual_gc\": actual_gc - total_gc,\n",
    "            \"residual_ac\": actual_ac - total_ac,\n",
    "            \"residual_sales\": actual_sales - total_sales_from_models,\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_reconciliation(\n",
    "    df: pd.DataFrame,\n",
    "    final_df: pd.DataFrame,\n",
    "    week_t: pd.Timestamp = None,\n",
    "    panel_method: str = \"fe\",\n",
    "    algorithm: str = \"ridge\",\n",
    "    algo_params_gc: dict = None,\n",
    "    algo_params_ac: dict = None\n",
    "):\n",
    "    if week_t is None:\n",
    "        week_t = df[\"week_start\"].max()\n",
    "\n",
    "    final_features = final_df[\"feature\"].tolist()\n",
    "\n",
    "    if algo_params_gc is None:\n",
    "        algo_params_gc = {\"alpha\": 10.0} if algorithm == \"ridge\" else {}\n",
    "    if algo_params_ac is None:\n",
    "        algo_params_ac = {\"alpha\": 10.0} if algorithm == \"ridge\" else {}\n",
    "\n",
    "    gc_fit, gc_coef = fit_champion_on_full_window(\n",
    "        df, \"log_gc\", final_features,\n",
    "        panel_method=panel_method, algorithm=algorithm, algo_params=algo_params_gc\n",
    "    )\n",
    "    ac_fit, ac_coef = fit_champion_on_full_window(\n",
    "        df, \"log_ac\", final_features,\n",
    "        panel_method=panel_method, algorithm=algorithm, algo_params=algo_params_ac\n",
    "    )\n",
    "\n",
    "    recon = reconcile_sales_yoy(\n",
    "        df=df, week_t=week_t, final_features=final_features,\n",
    "        gc_coef_table=gc_coef, ac_coef_table=ac_coef\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"final_features\": final_features,\n",
    "        \"gc_coef_table\": gc_coef,\n",
    "        \"ac_coef_table\": ac_coef,\n",
    "        \"driver_table\": recon[\"driver_table\"],\n",
    "        \"totals\": recon[\"totals\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baysian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_bayes_design(\n",
    "    df: pd.DataFrame,\n",
    "    y_col: str,\n",
    "    x_cols: list,\n",
    "    store_col: str = \"store_id\",\n",
    "    week_col: str = \"week_start\",\n",
    "    domain_map: dict = None,   # feature -> domain string\n",
    "):\n",
    "    df = df.sort_values([store_col, week_col]).copy()\n",
    "\n",
    "    # y, X\n",
    "    y = df[y_col].values.astype(float)\n",
    "    X = df[x_cols].values.astype(float)\n",
    "\n",
    "    # store index 0..n_store-1\n",
    "    store_codes, store_uniques = pd.factorize(df[store_col])\n",
    "    store_idx = store_codes.astype(int)\n",
    "\n",
    "    # week index (optional)\n",
    "    week_codes, week_uniques = pd.factorize(df[week_col])\n",
    "    week_idx = week_codes.astype(int)\n",
    "\n",
    "    # domain index 0..n_domain-1 for each feature column\n",
    "    if domain_map is None:\n",
    "        # fallback: infer by prefix\n",
    "        def infer_dom(f):\n",
    "            for d in [\"digital_promo\",\"nondigital_promo\",\"media\",\"menu\",\"mix_share\",\"csat\",\"pricing\"]:\n",
    "                if f.startswith(d + \"_\"):\n",
    "                    return d\n",
    "            return \"other\"\n",
    "        domain_map = {f: infer_dom(f) for f in x_cols}\n",
    "\n",
    "    domains = sorted(list(set(domain_map.values())))\n",
    "    dom_to_id = {d:i for i,d in enumerate(domains)}\n",
    "    feat_domain_idx = np.array([dom_to_id[domain_map[f]] for f in x_cols], dtype=int)\n",
    "\n",
    "    return {\n",
    "        \"y\": y,\n",
    "        \"X\": X,\n",
    "        \"x_cols\": x_cols,\n",
    "        \"store_idx\": store_idx,\n",
    "        \"n_store\": len(store_uniques),\n",
    "        \"week_idx\": week_idx,\n",
    "        \"n_week\": len(week_uniques),\n",
    "        \"feat_domain_idx\": feat_domain_idx,\n",
    "        \"domains\": domains\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "def fit_hierarchical_bayes(\n",
    "    design: dict,\n",
    "    include_week_effect: bool = False,\n",
    "    draws: int = 1000,\n",
    "    tune: int = 1000,\n",
    "    chains: int = 4,\n",
    "    target_accept: float = 0.9,\n",
    "    random_seed: int = 42,\n",
    "):\n",
    "    y = design[\"y\"]\n",
    "    X = design[\"X\"]\n",
    "    store_idx = design[\"store_idx\"]\n",
    "    n_store = design[\"n_store\"]\n",
    "    feat_dom = design[\"feat_domain_idx\"]\n",
    "    n_domain = len(design[\"domains\"])\n",
    "\n",
    "    if include_week_effect:\n",
    "        week_idx = design[\"week_idx\"]\n",
    "        n_week = design[\"n_week\"]\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # ---------- Random intercepts (store) ----------\n",
    "        mu_a = pm.Normal(\"mu_a\", mu=0.0, sigma=2.0)\n",
    "        sigma_a = pm.HalfNormal(\"sigma_a\", sigma=1.0)\n",
    "        a_store = pm.Normal(\"a_store\", mu=mu_a, sigma=sigma_a, shape=n_store)\n",
    "\n",
    "        # ---------- Domain-level shrinkage for betas ----------\n",
    "        # tau_d controls how large coefficients in domain d can be\n",
    "        tau_domain = pm.HalfNormal(\"tau_domain\", sigma=0.5, shape=n_domain)\n",
    "\n",
    "        # coefficient per feature, variance depends on its domain\n",
    "        beta = pm.Normal(\"beta\", mu=0.0, sigma=tau_domain[feat_dom], shape=X.shape[1])\n",
    "\n",
    "        # ---------- Optional week effect ----------\n",
    "        if include_week_effect:\n",
    "            sigma_w = pm.HalfNormal(\"sigma_w\", sigma=0.5)\n",
    "            w = pm.Normal(\"w\", mu=0.0, sigma=sigma_w, shape=n_week)\n",
    "            mu = a_store[store_idx] + w[week_idx] + pm.math.dot(X, beta)\n",
    "        else:\n",
    "            mu = a_store[store_idx] + pm.math.dot(X, beta)\n",
    "\n",
    "        # ---------- Observation noise ----------\n",
    "        sigma = pm.HalfNormal(\"sigma\", sigma=1.0)\n",
    "        y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y)\n",
    "\n",
    "        # ---------- Sample ----------\n",
    "        idata = pm.sample(\n",
    "            draws=draws, tune=tune, chains=chains,\n",
    "            target_accept=target_accept,\n",
    "            random_seed=random_seed,\n",
    "            return_inferencedata=True\n",
    "        )\n",
    "\n",
    "    return model, idata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_feature_table(idata, x_cols, eps: float = 0.001, prob_threshold: float = 0.8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a coefficient table similar to your frequentist outputs.\n",
    "    Uses posterior to compute sign stability and selection probability.\n",
    "    \"\"\"\n",
    "    beta_samples = idata.posterior[\"beta\"].stack(sample=(\"chain\",\"draw\")).values  # shape: (p, n_samples)\n",
    "    # PyMC/ArviZ may give shape (chain, draw, p) so we transpose if needed\n",
    "    if beta_samples.shape[0] != len(x_cols):\n",
    "        beta_samples = beta_samples.T\n",
    "\n",
    "    means = beta_samples.mean(axis=1)\n",
    "    sds = beta_samples.std(axis=1, ddof=0)\n",
    "\n",
    "    # Probabilities\n",
    "    p_pos = (beta_samples > 0).mean(axis=1)\n",
    "    p_neg = (beta_samples < 0).mean(axis=1)\n",
    "    p_nontrivial = (np.abs(beta_samples) > eps).mean(axis=1)\n",
    "\n",
    "    # credible intervals\n",
    "    q05 = np.quantile(beta_samples, 0.05, axis=1)\n",
    "    q50 = np.quantile(beta_samples, 0.50, axis=1)\n",
    "    q95 = np.quantile(beta_samples, 0.95, axis=1)\n",
    "\n",
    "    dfc = pd.DataFrame({\n",
    "        \"feature\": x_cols,\n",
    "        \"coef\": means,\n",
    "        \"coef_sd\": sds,\n",
    "        \"p_pos\": p_pos,\n",
    "        \"p_neg\": p_neg,\n",
    "        \"p_nontrivial\": p_nontrivial,\n",
    "        \"ci05\": q05,\n",
    "        \"ci50\": q50,\n",
    "        \"ci95\": q95,\n",
    "    })\n",
    "    dfc[\"sign\"] = np.where(dfc[\"coef\"] >= 0, 1, -1)\n",
    "    dfc[\"abs_coef\"] = dfc[\"coef\"].abs()\n",
    "    dfc[\"rank_abs\"] = dfc[\"abs_coef\"].rank(ascending=False, method=\"average\")\n",
    "\n",
    "    # Bayesian “selected” flag: nontrivial with high probability AND sign probability high\n",
    "    dfc[\"sign_prob\"] = dfc[[\"p_pos\",\"p_neg\"]].max(axis=1)\n",
    "    dfc[\"selected_flag\"] = ((dfc[\"p_nontrivial\"] >= prob_threshold) & (dfc[\"sign_prob\"] >= prob_threshold)).astype(int)\n",
    "\n",
    "    # For compatibility with your frequentist schema (no p-values here)\n",
    "    dfc[\"se\"] = np.nan\n",
    "    dfc[\"pval\"] = np.nan\n",
    "\n",
    "    return dfc.sort_values(\"abs_coef\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_experiment_hb(\n",
    "    df: pd.DataFrame,\n",
    "    y_col: str,\n",
    "    feature_cols: list,\n",
    "    split: tuple,\n",
    "    domain_map: dict = None,\n",
    "    include_week_effect: bool = False,\n",
    "    bayes_params: dict = None,\n",
    "):\n",
    "    if bayes_params is None:\n",
    "        bayes_params = dict(draws=600, tune=600, chains=4, target_accept=0.9)\n",
    "\n",
    "    train_start, train_end, test_start, test_end = split\n",
    "    df_train = df[(df[\"week_start\"] >= train_start) & (df[\"week_start\"] <= train_end)].copy()\n",
    "\n",
    "    design = build_bayes_design(\n",
    "        df_train, y_col=y_col, x_cols=feature_cols,\n",
    "        store_col=\"store_id\", week_col=\"week_start\",\n",
    "        domain_map=domain_map\n",
    "    )\n",
    "\n",
    "    model, idata = fit_hierarchical_bayes(\n",
    "        design,\n",
    "        include_week_effect=include_week_effect,\n",
    "        **bayes_params\n",
    "    )\n",
    "\n",
    "    coef_table = posterior_feature_table(idata, feature_cols, eps=0.001, prob_threshold=0.8)\n",
    "\n",
    "    # sanity metrics (Bayes doesn't emphasize these, but you may log them)\n",
    "    # Use posterior mean prediction for quick sanity:\n",
    "    # (Optional, keep minimal for now)\n",
    "\n",
    "    return {\n",
    "        \"target\": y_col,\n",
    "        \"panel_method\": \"hb_domain_shrinkage\" + (\"_week\" if include_week_effect else \"\"),\n",
    "        \"algorithm\": \"bayes\",\n",
    "        \"algo_params\": bayes_params,\n",
    "        \"train_start\": train_start,\n",
    "        \"train_end\": train_end,\n",
    "        \"test_start\": test_start,\n",
    "        \"test_end\": test_end,\n",
    "        \"coef_table\": coef_table,\n",
    "        \"idata\": idata,  # keep around if you want trace diagnostics\n",
    "        \"fit_metrics\": {}  # optional\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "import arviz as az\n",
    "\n",
    "def log_hb_run_to_mlflow(run_out: dict):\n",
    "    coef_table = run_out[\"coef_table\"]\n",
    "    idata = run_out.get(\"idata\", None)\n",
    "\n",
    "    with mlflow.start_run(run_name=run_out.get(\"run_name\", None), nested=True):\n",
    "        mlflow.log_param(\"target\", run_out[\"target\"])\n",
    "        mlflow.log_param(\"panel_method\", run_out[\"panel_method\"])\n",
    "        mlflow.log_param(\"algorithm\", run_out[\"algorithm\"])\n",
    "        mlflow.log_param(\"train_start\", str(run_out[\"train_start\"].date()))\n",
    "        mlflow.log_param(\"train_end\", str(run_out[\"train_end\"].date()))\n",
    "        mlflow.log_param(\"test_start\", str(run_out[\"test_start\"].date()))\n",
    "        mlflow.log_param(\"test_end\", str(run_out[\"test_end\"].date()))\n",
    "        mlflow.log_param(\"n_features\", coef_table.shape[0])\n",
    "\n",
    "        mlflow.log_text(json.dumps(run_out[\"algo_params\"], indent=2), \"bayes_params.json\")\n",
    "        mlflow.log_text(coef_table.to_csv(index=False), \"coef_table.csv\")\n",
    "\n",
    "        if idata is not None:\n",
    "            summ = az.summary(idata, var_names=[\"beta\"], round_to=6)\n",
    "            # log a couple aggregate diagnostics\n",
    "            mlflow.log_metric(\"beta_rhat_max\", float(summ[\"r_hat\"].max()))\n",
    "            mlflow.log_metric(\"beta_ess_bulk_min\", float(summ[\"ess_bulk\"].min()))\n",
    "            mlflow.log_text(summ.reset_index().to_csv(index=False), \"posterior_summary_beta.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
